{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VtEWtM_snfc4","executionInfo":{"status":"ok","timestamp":1764513958743,"user_tz":-360,"elapsed":46082,"user":{"displayName":"Emran Emon","userId":"04070448221491096863"}},"outputId":"d8ed85e2-492b-4c67-c362-fd4bee892a66"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.6/802.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hMounted at /content/drive\n","TensorFlow Version: 2.19.0\n","Keras Version (should be ~2.15 via tf_keras): 2.19.0\n"]}],"source":["!pip install tf-keras keras-cv-attention-models keras-tuner -q\n","\n","import os\n","import sys\n","\n","os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n","\n","import tensorflow as tf\n","import tf_keras\n","import numpy as np\n","import pandas as pd\n","import keras_tuner\n","\n","tf.keras = tf_keras\n","sys.modules[\"tensorflow.keras\"] = tf_keras\n","\n","from tf_keras import layers, Model\n","from tf_keras.optimizers import AdamW\n","from keras_cv_attention_models import maxvit\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from tf_keras import mixed_precision\n","policy = mixed_precision.Policy('mixed_float16')\n","mixed_precision.set_global_policy(policy)\n","\n","import gc\n","\n","drive.mount('/content/drive')\n","\n","print(\"TensorFlow Version:\", tf.__version__)\n","print(\"Keras Version (should be ~2.15 via tf_keras):\", tf_keras.__version__)"]},{"cell_type":"code","source":["IMG_SIZE = (224, 224)\n","BATCH_SIZE = 16\n","CLASSES = [\"acne\", \"pigmentation\", \"wrinkles\"]\n","DATA_ROOT = \"/content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles\"\n","\n","# Define the model path here so it is available globally\n","FINAL_MODEL_PATH = os.path.join(DATA_ROOT, \"maxvit_tiny_skin_model_FINAL_TUNED.keras\")"],"metadata":{"id":"Q2HQsFPGoa5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(os.path.join(DATA_ROOT, \"labels.csv\"))\n","df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.join(DATA_ROOT, x))\n","\n","# Stratified split to keep class balance\n","train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df[CLASSES])\n","train_df, val_df = train_test_split(train_val_df, test_size=0.15, random_state=42, stratify=train_val_df[CLASSES])\n","\n","pos_counts = train_df[CLASSES].sum().values\n","total_train_samples = len(train_df)\n","\n","print(f\"Training samples: {len(train_df)}\")\n","print(f\"Validation samples: {len(val_df)}\")\n","print(f\"Test samples: {len(test_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPganbYIodIC","executionInfo":{"status":"ok","timestamp":1764513973991,"user_tz":-360,"elapsed":15237,"user":{"displayName":"Emran Emon","userId":"04070448221491096863"}},"outputId":"3d19adc0-3839-4663-898c-e2da7f7b52d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples: 3656\n","Validation samples: 646\n","Test samples: 760\n"]}]},{"cell_type":"code","source":["data_augmentation = tf.keras.Sequential([\n","    layers.RandomFlip(\"horizontal\"),\n","    layers.RandomRotation(0.3),\n","    layers.RandomZoom(0.3),\n","    layers.RandomContrast(0.2),\n","], name=\"data_augmentation\")\n","\n","def parse_function(filename, labels):\n","    image_string = tf.io.read_file(filename)\n","    image_decoded = tf.io.decode_jpeg(image_string, channels=3)\n","    # Convert to float32 in [0, 1] range\n","    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n","    image_resized = tf.image.resize(image, IMG_SIZE)\n","    return image_resized, labels\n","\n","def create_dataset(df, batch_size, augment=False, cache_file=None):\n","    dataset = tf.data.Dataset.from_tensor_slices(\n","        (df[\"filename\"].values, df[CLASSES].values.astype(np.float32))\n","    )\n","    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    if augment:\n","        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n","                              num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    if cache_file:\n","        dataset = dataset.cache(cache_file)\n","    else:\n","        dataset = dataset.cache()\n","\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset\n","\n","# Define cache paths\n","train_cache_file = os.path.join(DATA_ROOT, 'maxvit_train_cache')\n","val_cache_file = os.path.join(DATA_ROOT, 'maxvit_val_cache')\n","\n","# Create datasets\n","train_ds = create_dataset(train_df, BATCH_SIZE, augment=True, cache_file=train_cache_file)\n","val_ds = create_dataset(val_df, BATCH_SIZE, augment=False, cache_file=val_cache_file)\n","test_ds = create_dataset(test_df, BATCH_SIZE, augment=False)\n","\n","print(\"Data pipelines created.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfzaUlsvogIS","executionInfo":{"status":"ok","timestamp":1764513975139,"user_tz":-360,"elapsed":1106,"user":{"displayName":"Emran Emon","userId":"04070448221491096863"}},"outputId":"d802cdad-94f1-4150-8ca5-1d3bfadabc12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data pipelines created.\n"]}]},{"cell_type":"code","source":["def create_weighted_bce_loss(pos_counts, total_samples, smooth=0.05):\n","    pos = tf.constant(pos_counts, dtype=tf.float32)\n","    neg = total_samples - pos\n","    w_pos = neg / tf.maximum(pos, 1.0)\n","    w_neg = tf.ones_like(pos)\n","\n","    def weighted_bce(y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","        y_true = y_true * (1.0 - smooth) + 0.5 * smooth\n","        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n","        weights = y_true * w_pos + (1.0 - y_true) * w_neg\n","        return tf.reduce_mean(bce * weights)\n","\n","    return weighted_bce\n","\n","loss_fn = create_weighted_bce_loss(pos_counts, total_train_samples)\n","custom_objects = {\"weighted_bce\": loss_fn}"],"metadata":{"id":"lq2vtTmnoh0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_hyper_model(hp):\n","    hp_dropout = hp.Float('dropout', 0.2, 0.5, step=0.1)\n","\n","    # This bridges the gap between your float32 dataset and the float16 model\n","    inputs = layers.Input(shape=IMG_SIZE + (3,), dtype='float32')\n","\n","    # Initialize MaxViT Tiny\n","    # Note: We must also force the base model to use float32 inputs if needed,\n","    # but usually connecting it to the float32 input tensor is enough.\n","    base_model = maxvit.MaxViT_Tiny(\n","        input_shape=IMG_SIZE + (3,),\n","        pretrained=\"imagenet\",\n","        num_classes=0\n","    )\n","\n","    # The base_model will automatically cast the float32 input to float16\n","    x = base_model(inputs)\n","    x = layers.GlobalAveragePooling2D()(x)\n","    x = layers.Dropout(hp_dropout)(x)\n","\n","    # Ensure output is float32 for stable loss calculation\n","    outputs = layers.Dense(len(CLASSES), activation=\"sigmoid\", dtype='float32')(x)\n","\n","    model = Model(inputs, outputs, name=\"maxvit_tiny_hyper_model\")\n","    return model"],"metadata":{"id":"0GOeNIUTojwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomTuner(keras_tuner.RandomSearch):\n","    def __init__(self, loss_function, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_function = loss_function\n","\n","    def run_trial(self, trial, train_ds, val_ds, **kwargs):\n","        tf.keras.backend.clear_session()\n","        gc.collect()\n","        hp = trial.hyperparameters\n","        model = self.hypermodel.build(hp)\n","\n","        all_metrics = [\n","            tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n","            tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n","            tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n","            tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n","        ]\n","\n","        # STAGE 1: Train Head Only\n","        print(f\"\\n[Trial {trial.trial_id}] Stage 1: Training head...\")\n","        head_lr = hp.Float('head_lr', 1e-4, 1e-3, sampling='log')\n","\n","        # Freeze the MaxViT base layers.\n","        # In this library, the base is often the whole model except our new head.\n","        # We can freeze layers by index since we know we added layers at the end.\n","        # Freezing everything except the last 3 layers (GAP, Dropout, Dense)\n","        for layer in model.layers[:-3]:\n","             layer.trainable = False\n","\n","        model.compile(\n","            optimizer=AdamW(learning_rate=head_lr, weight_decay=1e-4),\n","            loss=self.loss_function,\n","            metrics=all_metrics\n","        )\n","\n","        model.fit(train_ds, validation_data=val_ds, epochs=8, verbose=1)\n","\n","        # STAGE 2: Fine-Tuning\n","        print(f\"\\n[Trial {trial.trial_id}] Stage 2: Fine-tuning...\")\n","        finetune_lr = hp.Float('finetune_lr', 1e-6, 5e-5, sampling='log')\n","\n","        # Unfreeze all\n","        for layer in model.layers:\n","            layer.trainable = True\n","\n","        model.compile(\n","            optimizer=AdamW(learning_rate=finetune_lr, weight_decay=1e-4),\n","            loss=self.loss_function,\n","            metrics=all_metrics\n","        )\n","\n","        callbacks = [\n","            tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True),\n","            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.2, patience=2)\n","        ]\n","\n","        model.fit(\n","            train_ds,\n","            validation_data=val_ds,\n","            epochs=40,\n","            callbacks=callbacks,\n","            initial_epoch=8,\n","            verbose=1\n","        )\n","\n","        eval_results = model.evaluate(val_ds, return_dict=True, verbose=0)\n","        return {f\"val_{k}\": v for k, v in eval_results.items()}"],"metadata":{"id":"LnZAxisHopaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","print(\"Best HPs found:\", best_hps.values)\n","\n","final_model = build_hyper_model(best_hps)\n","final_model.summary()\n","\n","final_callbacks = [\n","    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=7, restore_best_weights=True),\n","    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.2, patience=3, min_lr=1e-6),\n","    tf.keras.callbacks.ModelCheckpoint(FINAL_MODEL_PATH, monitor=\"val_auc\", mode=\"max\", save_best_only=True)\n","]\n","\n","print(\"\\n--- Final Training Stage 1 ---\")\n","for layer in final_model.layers[:-3]:\n","     layer.trainable = False\n","\n","final_model.compile(\n","    optimizer=AdamW(learning_rate=best_hps.get('head_lr'), weight_decay=1e-4),\n","    loss=loss_fn,\n","    metrics=[tf.keras.metrics.BinaryAccuracy(name=\"acc\"), tf.keras.metrics.AUC(name=\"auc\", multi_label=True), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",")\n","history_head = final_model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=1)\n","\n","print(\"\\n--- Final Training Stage 2 ---\")\n","for layer in final_model.layers:\n","    layer.trainable = True\n","\n","final_model.compile(\n","    optimizer=AdamW(learning_rate=best_hps.get('finetune_lr'), weight_decay=1e-4),\n","    loss=loss_fn,\n","    metrics=[tf.keras.metrics.BinaryAccuracy(name=\"acc\"), tf.keras.metrics.AUC(name=\"auc\", multi_label=True), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",")\n","history_fine_tune = final_model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=70,\n","    callbacks=final_callbacks,\n","    initial_epoch=len(history_head.history['loss']),\n","    verbose=1\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2S5T6Cmtouwu","outputId":"17664eaa-193b-4604-aa03-f11b9e83eab7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best HPs found: {'dropout': 0.30000000000000004, 'head_lr': 0.0007015772037865249, 'finetune_lr': 2.937665256971339e-05}\n",">>>> Load pretrained from: /root/.keras/models/maxvit_tiny_224_imagenet.h5\n","Model: \"maxvit_tiny_hyper_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," maxvit_tiny (Functional)    (None, 7, 7, 512)         30187464  \n","                                                                 \n"," global_average_pooling2d_1  (None, 512)               0         \n","  (GlobalAveragePooling2D)                                       \n","                                                                 \n"," dropout_1 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 3)                 1539      \n","                                                                 \n","=================================================================\n","Total params: 30189003 (115.16 MB)\n","Trainable params: 30141387 (114.98 MB)\n","Non-trainable params: 47616 (186.00 KB)\n","_________________________________________________________________\n","\n","--- Final Training Stage 1 ---\n","Epoch 1/10\n","229/229 [==============================] - 93s 232ms/step - loss: 3.0841 - acc: 0.7477 - auc: 0.7947 - precision: 0.4175 - recall: 0.7433 - val_loss: 0.8709 - val_acc: 0.9004 - val_auc: 0.9595 - val_precision: 0.6810 - val_recall: 0.9206\n","Epoch 2/10\n","229/229 [==============================] - 48s 210ms/step - loss: 1.2222 - acc: 0.8704 - auc: 0.9268 - precision: 0.6197 - recall: 0.8677 - val_loss: 0.7190 - val_acc: 0.9123 - val_auc: 0.9742 - val_precision: 0.7080 - val_recall: 0.9365\n","Epoch 3/10\n","229/229 [==============================] - 47s 205ms/step - loss: 0.9614 - acc: 0.8906 - auc: 0.9472 - precision: 0.6630 - recall: 0.8929 - val_loss: 0.6545 - val_acc: 0.9309 - val_auc: 0.9800 - val_precision: 0.7652 - val_recall: 0.9312\n","Epoch 4/10\n","229/229 [==============================] - 46s 203ms/step - loss: 0.8654 - acc: 0.9031 - auc: 0.9562 - precision: 0.6924 - recall: 0.9051 - val_loss: 0.6043 - val_acc: 0.9365 - val_auc: 0.9837 - val_precision: 0.7802 - val_recall: 0.9392\n","Epoch 5/10\n","229/229 [==============================] - 47s 205ms/step - loss: 0.7615 - acc: 0.9121 - auc: 0.9627 - precision: 0.7139 - recall: 0.9168 - val_loss: 0.5150 - val_acc: 0.9438 - val_auc: 0.9871 - val_precision: 0.8009 - val_recall: 0.9471\n","Epoch 6/10\n","229/229 [==============================] - 47s 205ms/step - loss: 0.6700 - acc: 0.9220 - auc: 0.9700 - precision: 0.7414 - recall: 0.9219 - val_loss: 0.5120 - val_acc: 0.9561 - val_auc: 0.9881 - val_precision: 0.8582 - val_recall: 0.9286\n","Epoch 7/10\n","229/229 [==============================] - 47s 204ms/step - loss: 0.6646 - acc: 0.9152 - auc: 0.9692 - precision: 0.7231 - recall: 0.9158 - val_loss: 0.4810 - val_acc: 0.9577 - val_auc: 0.9891 - val_precision: 0.8663 - val_recall: 0.9259\n","Epoch 8/10\n","229/229 [==============================] - 47s 207ms/step - loss: 0.6099 - acc: 0.9308 - auc: 0.9745 - precision: 0.7638 - recall: 0.9341 - val_loss: 0.4352 - val_acc: 0.9577 - val_auc: 0.9912 - val_precision: 0.8491 - val_recall: 0.9524\n","Epoch 9/10\n","229/229 [==============================] - 47s 203ms/step - loss: 0.5533 - acc: 0.9301 - auc: 0.9780 - precision: 0.7630 - recall: 0.9303 - val_loss: 0.4424 - val_acc: 0.9510 - val_auc: 0.9897 - val_precision: 0.8223 - val_recall: 0.9550\n","Epoch 10/10\n","229/229 [==============================] - 47s 205ms/step - loss: 0.5170 - acc: 0.9356 - auc: 0.9805 - precision: 0.7767 - recall: 0.9402 - val_loss: 0.4196 - val_acc: 0.9603 - val_auc: 0.9913 - val_precision: 0.8662 - val_recall: 0.9418\n","\n","--- Final Training Stage 2 ---\n","Epoch 11/70\n","229/229 [==============================] - 272s 817ms/step - loss: 0.5523 - acc: 0.9350 - auc: 0.9780 - precision: 0.7751 - recall: 0.9392 - val_loss: 0.3359 - val_acc: 0.9799 - val_auc: 0.9964 - val_precision: 0.9403 - val_recall: 0.9577 - lr: 2.9377e-05\n","Epoch 12/70\n","229/229 [==============================] - 124s 539ms/step - loss: 0.3304 - acc: 0.9756 - auc: 0.9970 - precision: 0.9069 - recall: 0.9748 - val_loss: 0.3152 - val_acc: 0.9809 - val_auc: 0.9972 - val_precision: 0.9383 - val_recall: 0.9656 - lr: 2.9377e-05\n","Epoch 13/70\n","229/229 [==============================] - 139s 609ms/step - loss: 0.2771 - acc: 0.9903 - auc: 0.9995 - precision: 0.9589 - recall: 0.9930 - val_loss: 0.2921 - val_acc: 0.9866 - val_auc: 0.9982 - val_precision: 0.9560 - val_recall: 0.9762 - lr: 2.9377e-05\n","Epoch 14/70\n","229/229 [==============================] - 186s 812ms/step - loss: 0.2568 - acc: 0.9954 - auc: 0.9999 - precision: 0.9802 - recall: 0.9967 - val_loss: 0.2861 - val_acc: 0.9871 - val_auc: 0.9985 - val_precision: 0.9608 - val_recall: 0.9735 - lr: 2.9377e-05\n","Epoch 15/70\n","229/229 [==============================] - 191s 833ms/step - loss: 0.2454 - acc: 0.9971 - auc: 0.9999 - precision: 0.9875 - recall: 0.9977 - val_loss: 0.2766 - val_acc: 0.9881 - val_auc: 0.9988 - val_precision: 0.9634 - val_recall: 0.9762 - lr: 2.9377e-05\n","Epoch 16/70\n","229/229 [==============================] - 258s 1s/step - loss: 0.2360 - acc: 0.9984 - auc: 1.0000 - precision: 0.9930 - recall: 0.9986 - val_loss: 0.2669 - val_acc: 0.9897 - val_auc: 0.9989 - val_precision: 0.9637 - val_recall: 0.9841 - lr: 2.9377e-05\n","Epoch 17/70\n"," 17/229 [=>............................] - ETA: 1:38 - loss: 0.2325 - acc: 0.9988 - auc: 1.0000 - precision: 0.9937 - recall: 1.0000"]}]},{"cell_type":"code","source":["print(f\"\\nLoading best saved final model from: {FINAL_MODEL_PATH}\")\n","loaded_best_model = tf.keras.models.load_model(FINAL_MODEL_PATH, custom_objects=custom_objects)\n","\n","print(\"\\nEvaluating the final tuned MaxViT-Tiny model on the test set...\")\n","test_results = loaded_best_model.evaluate(test_ds, return_dict=True)\n","\n","print(\"\\nFinal MaxViT-Tiny Test Set Results\")\n","for metric, value in test_results.items():\n","    print(f\"{metric}: {value:.4f}\")\n","\n","precision = test_results.get('precision', 0.0)\n","recall = test_results.get('recall', 0.0)\n","if precision + recall > 0:\n","    f1 = 2 * (precision * recall) / (precision + recall)\n","    print(f\"F1 Score: {f1:.4f}\")\n","else:\n","    print(\"F1 Score: 0.0\")"],"metadata":{"id":"Z1FYpWf2sfYV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764515109755,"user_tz":-360,"elapsed":417097,"user":{"displayName":"Chadne Saiqa 2131124642","userId":"09857385317711847701"}},"outputId":"80143f1a-9256-40be-c50c-397d512a1b10"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading best saved final model from: /content/drive/MyDrive/acne clean pigmentation wrinkles/maxvit_tiny_skin_model_FINAL_TUNED.keras\n","\n","Evaluating the final tuned MaxViT-Tiny model on the test set...\n","48/48 [==============================] - 386s 8s/step - loss: 0.2564 - acc: 0.9904 - auc: 0.9996 - precision: 0.9628 - recall: 0.9888\n","\n","Final MaxViT-Tiny Test Set Results\n","loss: 0.2564\n","acc: 0.9904\n","auc: 0.9996\n","precision: 0.9628\n","recall: 0.9888\n","F1 Score: 0.9756\n"]}]},{"cell_type":"code","source":["import gc\n","import tensorflow as tf\n","# Ensure we use the legacy keras for compatibility\n","import tf_keras as keras\n","from tf_keras import layers, Model\n","from tf_keras.optimizers import AdamW\n","\n","# --- FIX 1: REDUCE BATCH SIZE TO PREVENT CRASHES ---\n","# Fine-tuning MaxViT is memory intensive. We must lower batch size to 16.\n","BATCH_SIZE = 16\n","\n","print(f\"Re-creating datasets with Batch Size: {BATCH_SIZE}...\")\n","# We assume create_dataset, train_df, val_df are already defined from previous cells\n","train_ds = create_dataset(train_df, BATCH_SIZE, augment=True, cache_file=os.path.join(DATA_ROOT, 'maxvit_train_cache_v2'))\n","val_ds = create_dataset(val_df, BATCH_SIZE, augment=False, cache_file=os.path.join(DATA_ROOT, 'maxvit_val_cache_v2'))\n","\n","# --- FIX 2: MODEL INPUT TYPE FIX ---\n","def build_hyper_model(hp):\n","    hp_dropout = hp.Float('dropout', 0.2, 0.5, step=0.1)\n","\n","    # EXPLICITLY set dtype='float32'.\n","    # This tells the model: \"Expect standard 32-bit images from the dataset\"\n","    # The mixed_precision policy will automatically cast them to float16 *after* this layer.\n","    inputs = layers.Input(shape=IMG_SIZE + (3,), dtype='float32')\n","\n","    base_model = maxvit.MaxViT_Tiny(\n","        input_shape=IMG_SIZE + (3,),\n","        pretrained=\"imagenet\",\n","        num_classes=0\n","    )\n","\n","    x = base_model(inputs)\n","    x = layers.GlobalAveragePooling2D()(x)\n","    x = layers.Dropout(hp_dropout)(x)\n","\n","    # Output layer stays float32 for stability\n","    outputs = layers.Dense(len(CLASSES), activation=\"sigmoid\", dtype='float32')(x)\n","\n","    model = Model(inputs, outputs, name=\"maxvit_tiny_hyper_model\")\n","    return model\n","\n","# --- FIX 3: MEMORY CLEANING TUNER ---\n","class CustomTuner(keras_tuner.RandomSearch):\n","    def __init__(self, loss_function, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.loss_function = loss_function\n","\n","    def run_trial(self, trial, train_ds, val_ds, **kwargs):\n","        # FORCE CLEAN GPU MEMORY BEFORE EACH TRIAL\n","        tf.keras.backend.clear_session()\n","        gc.collect()\n","\n","        hp = trial.hyperparameters\n","        model = self.hypermodel.build(hp)\n","\n","        # Standard metrics\n","        metrics = [\n","            tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n","            tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n","            tf.keras.metrics.Precision(name=\"precision\"),\n","            tf.keras.metrics.Recall(name=\"recall\")\n","        ]\n","\n","        # STAGE 1: Train Head\n","        print(f\"\\n[Trial {trial.trial_id}] Stage 1: Training head...\")\n","        # Freeze backbone\n","        for layer in model.layers[:-3]:\n","             layer.trainable = False\n","\n","        model.compile(\n","            optimizer=AdamW(learning_rate=hp.Float('head_lr', 1e-4, 1e-3, sampling='log'), weight_decay=1e-4),\n","            loss=self.loss_function,\n","            metrics=metrics\n","        )\n","        model.fit(train_ds, validation_data=val_ds, epochs=8, verbose=1)\n","\n","        # STAGE 2: Fine-Tuning\n","        print(f\"\\n[Trial {trial.trial_id}] Stage 2: Fine-tuning...\")\n","        # Unfreeze all\n","        for layer in model.layers:\n","            layer.trainable = True\n","\n","        model.compile(\n","            optimizer=AdamW(learning_rate=hp.Float('finetune_lr', 1e-6, 5e-5, sampling='log'), weight_decay=1e-4),\n","            loss=self.loss_function,\n","            metrics=metrics\n","        )\n","\n","        # Callbacks for fine-tuning\n","        callbacks = [\n","            tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True),\n","            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.2, patience=2)\n","        ]\n","\n","        model.fit(\n","            train_ds,\n","            validation_data=val_ds,\n","            epochs=40,\n","            initial_epoch=8,\n","            callbacks=callbacks,\n","            verbose=1\n","        )\n","\n","        return {f\"val_{k}\": v for k, v in model.evaluate(val_ds, return_dict=True, verbose=0).items()}\n","\n","# --- RUN THE TUNER ---\n","print(\"Starting Tuner with Fixes...\")\n","tuner = CustomTuner(\n","    loss_function=loss_fn,\n","    hypermodel=build_hyper_model,\n","    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n","    max_trials=4,\n","    executions_per_trial=1,\n","    directory=os.path.join(DATA_ROOT, 'keras_tuner_v2'), # New directory to avoid conflicts\n","    project_name='maxvit_tiny_tuning_fixed',\n","    overwrite=False\n",")\n","\n","tuner.search(train_ds=train_ds, val_ds=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_KEg2CQ4_15O","outputId":"13b6964f-1448-4fd7-c960-6f68caac414b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Re-creating datasets with Batch Size: 16...\n","Starting Tuner with Fixes...\n","Reloading Tuner from /content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/keras_tuner_v2/maxvit_tiny_tuning_fixed/tuner0.json\n","\n","Search: Running Trial #2\n","\n","Value             |Best Value So Far |Hyperparameter\n","0.4               |0.3               |dropout\n","0.00067958        |0.0001            |head_lr\n","7.0373e-06        |1e-06             |finetune_lr\n","\n","Downloading data from https://github.com/leondgarse/keras_cv_attention_models/releases/download/maxvit/maxvit_tiny_224_imagenet.h5\n","126113280/126113280 [==============================] - 1s 0us/step\n",">>>> Load pretrained from: /root/.keras/models/maxvit_tiny_224_imagenet.h5\n","\n","[Trial 1] Stage 1: Training head...\n","Epoch 1/8\n","229/229 [==============================] - 111s 313ms/step - loss: 5.7828 - acc: 0.6760 - auc: 0.7000 - precision: 0.3374 - recall: 0.6863 - val_loss: 1.0804 - val_acc: 0.9056 - val_auc: 0.9564 - val_precision: 0.7070 - val_recall: 0.8810\n","Epoch 2/8\n","229/229 [==============================] - 47s 205ms/step - loss: 1.7276 - acc: 0.8506 - auc: 0.8981 - precision: 0.5803 - recall: 0.8443 - val_loss: 0.8752 - val_acc: 0.9257 - val_auc: 0.9699 - val_precision: 0.7623 - val_recall: 0.8995\n","Epoch 3/8\n","229/229 [==============================] - 47s 204ms/step - loss: 1.3432 - acc: 0.8734 - auc: 0.9247 - precision: 0.6257 - recall: 0.8728 - val_loss: 0.7417 - val_acc: 0.9360 - val_auc: 0.9783 - val_precision: 0.7873 - val_recall: 0.9206\n","Epoch 4/8\n","229/229 [==============================] - 47s 204ms/step - loss: 1.0984 - acc: 0.8858 - auc: 0.9406 - precision: 0.6525 - recall: 0.8873 - val_loss: 0.8424 - val_acc: 0.8922 - val_auc: 0.9787 - val_precision: 0.6597 - val_recall: 0.9233\n","Epoch 5/8\n","229/229 [==============================] - 46s 202ms/step - loss: 0.9756 - acc: 0.8938 - auc: 0.9499 - precision: 0.6715 - recall: 0.8915 - val_loss: 0.7942 - val_acc: 0.9014 - val_auc: 0.9795 - val_precision: 0.6859 - val_recall: 0.9127\n","Epoch 6/8\n","229/229 [==============================] - 47s 203ms/step - loss: 0.8232 - acc: 0.9049 - auc: 0.9603 - precision: 0.6981 - recall: 0.9028 - val_loss: 0.6355 - val_acc: 0.9541 - val_auc: 0.9820 - val_precision: 0.8696 - val_recall: 0.8995\n","Epoch 7/8\n","229/229 [==============================] - 47s 203ms/step - loss: 0.7455 - acc: 0.9108 - auc: 0.9645 - precision: 0.7109 - recall: 0.9149 - val_loss: 0.5869 - val_acc: 0.9458 - val_auc: 0.9848 - val_precision: 0.8258 - val_recall: 0.9153\n","Epoch 8/8\n","229/229 [==============================] - 46s 202ms/step - loss: 0.7091 - acc: 0.9150 - auc: 0.9656 - precision: 0.7224 - recall: 0.9163 - val_loss: 0.5285 - val_acc: 0.9525 - val_auc: 0.9859 - val_precision: 0.8389 - val_recall: 0.9365\n","\n","[Trial 1] Stage 2: Fine-tuning...\n","Epoch 9/40\n","155/229 [===================>..........] - ETA: 56s - loss: 1.0377 - acc: 0.8917 - auc: 0.9476 - precision: 0.6663 - recall: 0.8833"]}]}]}