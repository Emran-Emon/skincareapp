{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7vfl8BF7OhA",
        "outputId": "361f9266-5da1-43a6-c31b-9df171b39e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "CLASSES = [\"acne\", \"pigmentation\", \"wrinkles\"]\n",
        "DATA_ROOT = \"/content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/\"\n",
        "\n",
        "# --- Load CSV and prepare file paths ---\n",
        "df = pd.read_csv(os.path.join(DATA_ROOT, \"labels.csv\"))\n",
        "\n",
        "# Create a 'filename' column with the full path to each image\n",
        "df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.join(DATA_ROOT, x))\n",
        "\n",
        "print(\"DataFrame Head:\")\n",
        "print(df.head())\n",
        "print(f\"\\nTotal images found: {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PIa-OvZ7wRA",
        "outputId": "c25bc6a6-b8f4-4f9f-c3a8-d3536356fcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Head:\n",
            "                                            filename  acne  wrinkles  \\\n",
            "0  /content/drive/MyDrive/skincareapp/acne clean ...     1         0   \n",
            "1  /content/drive/MyDrive/skincareapp/acne clean ...     1         0   \n",
            "2  /content/drive/MyDrive/skincareapp/acne clean ...     1         0   \n",
            "3  /content/drive/MyDrive/skincareapp/acne clean ...     1         0   \n",
            "4  /content/drive/MyDrive/skincareapp/acne clean ...     1         0   \n",
            "\n",
            "   pigmentation  clean  \n",
            "0             0      0  \n",
            "1             0      0  \n",
            "2             0      0  \n",
            "3             0      0  \n",
            "4             0      0  \n",
            "\n",
            "Total images found: 5062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=df[CLASSES]\n",
        ")\n",
        "\n",
        "# Second split to get the final training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.15,  # 15% of the remaining 85%\n",
        "    random_state=42,\n",
        "    stratify=train_val_df[CLASSES]\n",
        ")\n",
        "\n",
        "print(f\"Training samples:   {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Test samples:       {len(test_df)}\")\n",
        "\n",
        "\n",
        "#Calculate class weights for the custom loss function\n",
        "#These counts are from the original notebook and are used in the weighted BCE loss\n",
        "pos_counts = train_df[CLASSES].sum().values\n",
        "neg_counts = len(train_df) - pos_counts\n",
        "\n",
        "print(\"\\nPositive class counts (acne, pigmentation, wrinkles):\", pos_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCihOgNq8aKq",
        "outputId": "a0f49c48-6c43-4a87-f65d-8a3936a61f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples:   3656\n",
            "Validation samples: 646\n",
            "Test samples:       760\n",
            "\n",
            "Positive class counts (acne, pigmentation, wrinkles): [1015  386  738]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_function(filename, labels):\n",
        "    #Read the file\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    #Decode the image\n",
        "    image_decoded = tf.io.decode_jpeg(image_string, channels=3)\n",
        "    #Convert to float32\n",
        "    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
        "    #Resize the image\n",
        "    image_resized = tf.image.resize(image, IMG_SIZE)\n",
        "    return image_resized, labels\n",
        "\n",
        "def create_dataset(df, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (df[\"filename\"].values, df[CLASSES].values.astype(np.float32))\n",
        "    )\n",
        "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "#Create the datasets\n",
        "train_ds = create_dataset(train_df, BATCH_SIZE)\n",
        "val_ds = create_dataset(val_df, BATCH_SIZE)\n",
        "test_ds = create_dataset(test_df, BATCH_SIZE)\n",
        "\n",
        "print(\"\\n✅ tf.data pipelines created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7U74K2Q8iQr",
        "outputId": "854808b5-d366-4c77-c84a-f02063e72157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ tf.data pipelines created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters for the EANet Model\n",
        "PATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 256\n",
        "MLP_DIM = 512\n",
        "DIM_COEFFICIENT = 4\n",
        "NUM_HEADS = 8\n",
        "ATTENTION_DROPOUT = 0.2\n",
        "PROJECTION_DROPOUT = 0.2\n",
        "NUM_TRANSFORMER_BLOCKS = 4\n",
        "\n",
        "print(f\"Image Size: {IMG_SIZE[0]} X {IMG_SIZE[1]} = {IMG_SIZE[0] * IMG_SIZE[1]}\")\n",
        "print(f\"Patch size: {PATCH_SIZE} X {PATCH_SIZE} = {PATCH_SIZE**2} \")\n",
        "NUM_PATCHES = (IMG_SIZE[0] // PATCH_SIZE) ** 2\n",
        "print(f\"Patches per image: {NUM_PATCHES}\")\n",
        "\n",
        "\n",
        "# Custom Layer to Extract Patches\n",
        "class PatchExtractor(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        return patches\n",
        "\n",
        "    # This method tells Keras how to save the layer's configuration\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "\n",
        "#EANet Helper Modules\n",
        "class ExternalAttention(layers.Layer):\n",
        "    def __init__(self, dim, num_heads, dim_coefficient=4, attention_dropout=0.2, projection_dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_coefficient = dim_coefficient\n",
        "        self.linear_q = layers.Dense(dim * dim_coefficient)\n",
        "        self.linear_k = layers.Dense(dim * dim_coefficient)\n",
        "        self.linear_v = layers.Dense(dim * dim_coefficient)\n",
        "        self.linear_out = layers.Dense(dim)\n",
        "        self.softmax = layers.Softmax(axis=-1)\n",
        "        self.attention_drop = layers.Dropout(attention_dropout)\n",
        "        self.projection_drop = layers.Dropout(projection_dropout)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = self.linear_q(inputs)\n",
        "        k = self.linear_k(inputs)\n",
        "        v = self.linear_v(inputs)\n",
        "\n",
        "        q = tf.reshape(q, (-1, q.shape[1], self.num_heads, self.dim_coefficient))\n",
        "        k = tf.reshape(k, (-1, k.shape[1], self.num_heads, self.dim_coefficient))\n",
        "        v = tf.reshape(v, (-1, v.shape[1], self.num_heads, self.dim_coefficient))\n",
        "\n",
        "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "        k = tf.transpose(k, perm=[0, 2, 1, 3])\n",
        "        v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
        "\n",
        "        attention = self.softmax(tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(float(self.dim_coefficient)))\n",
        "        attention = self.attention_drop(attention)\n",
        "\n",
        "        out = tf.matmul(attention, v)\n",
        "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (-1, out.shape[1], self.dim * self.dim_coefficient))\n",
        "\n",
        "        out = self.linear_out(out)\n",
        "        out = self.projection_drop(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, mlp_dim, drop_rate=0.2, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = ExternalAttention(dim=embedding_dim, num_heads=num_heads)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Defining the MLP layers once inside __init__ using a Sequential model\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(mlp_dim, activation=tf.nn.gelu),\n",
        "            layers.Dropout(drop_rate),\n",
        "            layers.Dense(embedding_dim),\n",
        "            layers.Dropout(drop_rate)\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        attn_output = self.attention(self.norm1(x))\n",
        "        x = layers.add([x, attn_output])\n",
        "        mlp_output = self.mlp(self.norm2(x))\n",
        "        return layers.add([x, mlp_output])\n",
        "\n",
        "#Function to Build the Full EANet Model (No changes needed here)\n",
        "def build_eanet(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    patches = PatchExtractor(PATCH_SIZE)(inputs)\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = layers.Reshape((NUM_PATCHES, patch_dims))(patches)\n",
        "    patch_embedding = layers.Dense(units=EMBEDDING_DIM)(patches)\n",
        "    positions = tf.range(start=0, limit=NUM_PATCHES, delta=1)\n",
        "    position_embedding = layers.Embedding(\n",
        "        input_dim=NUM_PATCHES, output_dim=EMBEDDING_DIM\n",
        "    )(positions)\n",
        "    x = patch_embedding + position_embedding\n",
        "\n",
        "    for _ in range(NUM_TRANSFORMER_BLOCKS):\n",
        "        x = TransformerBlock(\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            num_heads=NUM_HEADS,\n",
        "            mlp_dim=MLP_DIM\n",
        "        )(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vap17TRk8sQv",
        "outputId": "07fe50af-9899-4d8f-dbb3-4f1167ef407b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Size: 224 X 224 = 50176\n",
            "Patch size: 16 X 16 = 256 \n",
            "Patches per image: 196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "tf.keras.backend.clear_session() # Clear previous models from memory\n",
        "eanet_model = build_eanet(\n",
        "    input_shape=IMG_SIZE + (3,),\n",
        "    num_classes=len(CLASSES)\n",
        ")\n",
        "eanet_model.summary()\n",
        "\n",
        "\n",
        "#Custom Weighted Binary Cross-Entropy Loss (from your notebook)\n",
        "def weighted_bce(y_true, y_pred, smooth=0.05):\n",
        "    y_true = y_true * (1.0 - smooth) + 0.5 * smooth\n",
        "    bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Using the calculated pos_counts from Cell 3\n",
        "    pos = tf.constant(pos_counts, dtype=tf.float32)\n",
        "    neg = len(train_df) - pos\n",
        "\n",
        "    w_pos = neg / tf.maximum(pos, 1.0)\n",
        "    w_neg = tf.ones_like(pos)\n",
        "\n",
        "    weights = y_true * w_pos + (1.0 - y_true) * w_neg\n",
        "    return tf.reduce_mean(bce * weights)\n",
        "\n",
        "#Compile the EANet model\n",
        "eanet_model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(1e-4, weight_decay=1e-5),\n",
        "    loss=weighted_bce,\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
        "        tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "#Callbacks\n",
        "EANET_MODEL_PATH = os.path.join(DATA_ROOT, \"eanet_skin_model.keras\")\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=7, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-6),\n",
        "    tf.keras.callbacks.ModelCheckpoint(EANET_MODEL_PATH, monitor=\"val_auc\", mode=\"max\", save_best_only=True)\n",
        "]\n",
        "\n",
        "#Train the model\n",
        "print(\"\\nStarting EANet model training...\")\n",
        "EPOCHS = 50\n",
        "history_eanet = eanet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Training complete. Best EANet model saved to {EANET_MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vhoiyf7j9BSm",
        "outputId": "97bc0a5e-3dc9-4839-8eca-c61a6690cceb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_extractor                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mPatchExtractor\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m196,864\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m1,315,840\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m1,315,840\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m1,315,840\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m1,315,840\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m771\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_extractor                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchExtractor</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,460,995\u001b[0m (20.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,460,995</span> (20.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,460,995\u001b[0m (20.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,460,995</span> (20.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting EANet model training...\n",
            "Epoch 1/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 812ms/step - acc: 0.7010 - auc: 0.7437 - loss: 1.0479 - precision: 0.3580 - recall: 0.6599 - val_acc: 0.7214 - val_auc: 0.8867 - val_loss: 0.9696 - val_precision: 0.3990 - val_recall: 0.8466 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 632ms/step - acc: 0.8074 - auc: 0.8774 - loss: 0.7838 - precision: 0.5023 - recall: 0.7941 - val_acc: 0.7183 - val_auc: 0.9022 - val_loss: 1.0183 - val_precision: 0.3953 - val_recall: 0.8386 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 634ms/step - acc: 0.8299 - auc: 0.9028 - loss: 0.7159 - precision: 0.5406 - recall: 0.8260 - val_acc: 0.7472 - val_auc: 0.9200 - val_loss: 0.9744 - val_precision: 0.4257 - val_recall: 0.8492 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 640ms/step - acc: 0.8545 - auc: 0.9218 - loss: 0.6643 - precision: 0.5859 - recall: 0.8540 - val_acc: 0.7595 - val_auc: 0.9280 - val_loss: 0.9638 - val_precision: 0.4371 - val_recall: 0.8095 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 640ms/step - acc: 0.8726 - auc: 0.9343 - loss: 0.6229 - precision: 0.6228 - recall: 0.8686 - val_acc: 0.7792 - val_auc: 0.9378 - val_loss: 0.9992 - val_precision: 0.4607 - val_recall: 0.7751 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 630ms/step - acc: 0.8797 - auc: 0.9427 - loss: 0.5928 - precision: 0.6395 - recall: 0.8690 - val_acc: 0.7972 - val_auc: 0.9450 - val_loss: 1.0166 - val_precision: 0.4876 - val_recall: 0.7778 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 632ms/step - acc: 0.8905 - auc: 0.9495 - loss: 0.5697 - precision: 0.6633 - recall: 0.8834 - val_acc: 0.8055 - val_auc: 0.9488 - val_loss: 1.0443 - val_precision: 0.5009 - val_recall: 0.7619 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 634ms/step - acc: 0.9027 - auc: 0.9548 - loss: 0.5498 - precision: 0.6925 - recall: 0.8949 - val_acc: 0.8240 - val_auc: 0.9526 - val_loss: 1.0029 - val_precision: 0.5343 - val_recall: 0.7619 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 626ms/step - acc: 0.9010 - auc: 0.9559 - loss: 0.5507 - precision: 0.6877 - recall: 0.8965 - val_acc: 0.8684 - val_auc: 0.9524 - val_loss: 0.9921 - val_precision: 0.6496 - val_recall: 0.7063 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 638ms/step - acc: 0.9140 - auc: 0.9631 - loss: 0.5160 - precision: 0.7227 - recall: 0.9041 - val_acc: 0.8710 - val_auc: 0.9637 - val_loss: 0.8546 - val_precision: 0.6416 - val_recall: 0.7672 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 636ms/step - acc: 0.9161 - auc: 0.9656 - loss: 0.5056 - precision: 0.7289 - recall: 0.9041 - val_acc: 0.9112 - val_auc: 0.9586 - val_loss: 0.9359 - val_precision: 0.8047 - val_recall: 0.7196 - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 633ms/step - acc: 0.9168 - auc: 0.9659 - loss: 0.5081 - precision: 0.7302 - recall: 0.9087 - val_acc: 0.8999 - val_auc: 0.9591 - val_loss: 0.9843 - val_precision: 0.7644 - val_recall: 0.7037 - learning_rate: 1.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 631ms/step - acc: 0.9256 - auc: 0.9734 - loss: 0.4718 - precision: 0.7550 - recall: 0.9143 - val_acc: 0.9185 - val_auc: 0.9665 - val_loss: 0.9115 - val_precision: 0.8459 - val_recall: 0.7116 - learning_rate: 1.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 629ms/step - acc: 0.9338 - auc: 0.9761 - loss: 0.4611 - precision: 0.7824 - recall: 0.9162 - val_acc: 0.9272 - val_auc: 0.9666 - val_loss: 0.8659 - val_precision: 0.8669 - val_recall: 0.7407 - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 642ms/step - acc: 0.9416 - auc: 0.9816 - loss: 0.4315 - precision: 0.8026 - recall: 0.9288 - val_acc: 0.9303 - val_auc: 0.9677 - val_loss: 0.8367 - val_precision: 0.8522 - val_recall: 0.7778 - learning_rate: 1.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 635ms/step - acc: 0.9392 - auc: 0.9811 - loss: 0.4367 - precision: 0.7914 - recall: 0.9345 - val_acc: 0.9340 - val_auc: 0.9665 - val_loss: 0.9267 - val_precision: 0.8981 - val_recall: 0.7460 - learning_rate: 1.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 627ms/step - acc: 0.9467 - auc: 0.9851 - loss: 0.4118 - precision: 0.8155 - recall: 0.9383 - val_acc: 0.9267 - val_auc: 0.9605 - val_loss: 1.0874 - val_precision: 0.8986 - val_recall: 0.7037 - learning_rate: 1.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 638ms/step - acc: 0.9494 - auc: 0.9880 - loss: 0.3968 - precision: 0.8285 - recall: 0.9335 - val_acc: 0.9396 - val_auc: 0.9713 - val_loss: 0.8819 - val_precision: 0.9040 - val_recall: 0.7725 - learning_rate: 1.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 642ms/step - acc: 0.9557 - auc: 0.9883 - loss: 0.3938 - precision: 0.8450 - recall: 0.9456 - val_acc: 0.9469 - val_auc: 0.9815 - val_loss: 0.6622 - val_precision: 0.8917 - val_recall: 0.8280 - learning_rate: 1.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 642ms/step - acc: 0.9609 - auc: 0.9921 - loss: 0.3643 - precision: 0.8593 - recall: 0.9557 - val_acc: 0.9582 - val_auc: 0.9816 - val_loss: 0.6168 - val_precision: 0.9381 - val_recall: 0.8413 - learning_rate: 1.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 645ms/step - acc: 0.9660 - auc: 0.9943 - loss: 0.3428 - precision: 0.8742 - recall: 0.9635 - val_acc: 0.9484 - val_auc: 0.9842 - val_loss: 0.6074 - val_precision: 0.8949 - val_recall: 0.8333 - learning_rate: 1.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 628ms/step - acc: 0.9694 - auc: 0.9952 - loss: 0.3341 - precision: 0.8854 - recall: 0.9673 - val_acc: 0.9536 - val_auc: 0.9725 - val_loss: 0.7150 - val_precision: 0.9235 - val_recall: 0.8307 - learning_rate: 1.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 621ms/step - acc: 0.9728 - auc: 0.9965 - loss: 0.3164 - precision: 0.8946 - recall: 0.9745 - val_acc: 0.9582 - val_auc: 0.9771 - val_loss: 0.6529 - val_precision: 0.9304 - val_recall: 0.8492 - learning_rate: 1.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 624ms/step - acc: 0.9722 - auc: 0.9951 - loss: 0.3272 - precision: 0.8926 - recall: 0.9741 - val_acc: 0.9572 - val_auc: 0.9795 - val_loss: 0.5730 - val_precision: 0.9019 - val_recall: 0.8757 - learning_rate: 1.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 622ms/step - acc: 0.9771 - auc: 0.9974 - loss: 0.3055 - precision: 0.9076 - recall: 0.9821 - val_acc: 0.9520 - val_auc: 0.9823 - val_loss: 0.5785 - val_precision: 0.8862 - val_recall: 0.8651 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 625ms/step - acc: 0.9826 - auc: 0.9982 - loss: 0.2912 - precision: 0.9288 - recall: 0.9858 - val_acc: 0.9592 - val_auc: 0.9824 - val_loss: 0.5732 - val_precision: 0.9235 - val_recall: 0.8624 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 634ms/step - acc: 0.9852 - auc: 0.9987 - loss: 0.2818 - precision: 0.9355 - recall: 0.9919 - val_acc: 0.9546 - val_auc: 0.9814 - val_loss: 0.6079 - val_precision: 0.9167 - val_recall: 0.8439 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 624ms/step - acc: 0.9835 - auc: 0.9988 - loss: 0.2807 - precision: 0.9321 - recall: 0.9871 - val_acc: 0.9572 - val_auc: 0.9827 - val_loss: 0.5748 - val_precision: 0.9178 - val_recall: 0.8571 - learning_rate: 2.5000e-05\n",
            "\n",
            "✅ Training complete. Best EANet model saved to /content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/eanet_skin_model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_objects = {\n",
        "    \"PatchExtractor\": PatchExtractor,\n",
        "    \"TransformerBlock\": TransformerBlock,\n",
        "    \"ExternalAttention\": ExternalAttention,\n",
        "    \"weighted_bce\": weighted_bce\n",
        "}\n",
        "\n",
        "#Loading the Model\n",
        "MODEL_PATH = \"/content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/eanet_skin_model.keras\"\n",
        "print(f\"Loading model from: {MODEL_PATH}\")\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(MODEL_PATH, custom_objects=custom_objects)\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "\n",
        "#Evaluation on the Test Set\n",
        "print(\"\\nEvaluating the final model on the unseen test set...\")\n",
        "test_results = loaded_model.evaluate(test_ds)\n",
        "\n",
        "print(\"\\nFinal Test Set Evaluation Results\")\n",
        "for metric, value in zip(loaded_model.metrics_names, test_results):\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UobEDVhaX1e",
        "outputId": "0daa2e63-6615-4089-f4c4-f3fd7b012a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/eanet_skin_model.keras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully!\n",
            "\n",
            "Evaluating the final model on the unseen test set...\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 11s/step - acc: 0.9492 - auc: 0.9793 - loss: 0.5812 - precision: 0.8852 - recall: 0.8398\n",
            "\n",
            "Final Test Set Evaluation Results\n",
            "loss: 0.6127\n",
            "compile_metrics: 0.9461\n"
          ]
        }
      ]
    }
  ]
}