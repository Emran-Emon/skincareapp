{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7vfl8BF7OhA",
        "outputId": "e2aef95a-6633-472b-8183-e5d8f69aaf5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "TensorFlow Version: 2.19.0\n",
            "KerasTuner Version: 1.4.7\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import keras_tuner\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "\n",
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#A function to set random seeds for reproducibility\n",
        "def set_seeds(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"KerasTuner Version:\", keras_tuner.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PIa-OvZ7wRA"
      },
      "outputs": [],
      "source": [
        "#Key parameters for the experiment\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "CLASSES = [\"acne\", \"pigmentation\", \"wrinkles\"]\n",
        "DATA_ROOT = \"/content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/\"\n",
        "\n",
        "#Constructs the full, absolute path for each image file\n",
        "df = pd.read_csv(os.path.join(DATA_ROOT, \"labels.csv\"))\n",
        "df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.join(DATA_ROOT, x))\n",
        "\n",
        "PATCH_SIZE = 32\n",
        "NUM_PATCHES = (IMG_SIZE[0] // PATCH_SIZE) ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCihOgNq8aKq",
        "outputId": "040ff8f1-5261-4c51-afa3-bd7aac562feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 3656, Val samples: 646, Test samples: 760\n",
            "Positive class counts in train set: [1015  386  738]\n"
          ]
        }
      ],
      "source": [
        "#Splitting data into training, validation, and test sets\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df[CLASSES])\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.15, random_state=42, stratify=train_val_df[CLASSES])\n",
        "\n",
        "#Calculating class counts for the weighted loss function\n",
        "pos_counts = train_df[CLASSES].sum().values\n",
        "total_train_samples = len(train_df)\n",
        "\n",
        "print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
        "print(f\"Positive class counts in train set: {pos_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7U74K2Q8iQr",
        "outputId": "e9088585-5ef2-411f-a807-e2559bd4f606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.data pipelines created successfully with disk caching enabled.\n"
          ]
        }
      ],
      "source": [
        "#Data Augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.3),\n",
        "    layers.RandomZoom(0.3),\n",
        "    layers.RandomContrast(0.2),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "#Create tf.data Pipelines\n",
        "def parse_function(filename, labels):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image_decoded = tf.io.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
        "    image_resized = tf.image.resize(image, IMG_SIZE)\n",
        "    return image_resized, labels\n",
        "\n",
        "def create_dataset(df, batch_size, augment=False, cache_file=None):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (df[\"filename\"].values, df[CLASSES].values.astype(np.float32))\n",
        "    )\n",
        "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
        "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    #Use disk caching to prevent RAM crashes\n",
        "    if cache_file:\n",
        "        dataset = dataset.cache(cache_file)\n",
        "    else:\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "#Create the datasets with disk caching\n",
        "train_cache_file = os.path.join(DATA_ROOT, 'train_cache_eanet')\n",
        "val_cache_file = os.path.join(DATA_ROOT, 'val_cache_eanet')\n",
        "\n",
        "train_ds = create_dataset(train_df, BATCH_SIZE, augment=True, cache_file=train_cache_file)\n",
        "val_ds = create_dataset(val_df, BATCH_SIZE, augment=False, cache_file=val_cache_file)\n",
        "test_ds = create_dataset(test_df, BATCH_SIZE, augment=False)\n",
        "\n",
        "print(\"tf.data pipelines created successfully with disk caching enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vap17TRk8sQv"
      },
      "outputs": [],
      "source": [
        "class PatchExtractor(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "class ExternalAttention(layers.Layer):\n",
        "    def __init__(self, dim, num_heads, dim_coefficient=4, attention_dropout=0.2, projection_dropout=0.2, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_coefficient = dim_coefficient\n",
        "        self.linear_q = layers.Dense(dim * dim_coefficient)\n",
        "        self.linear_k = layers.Dense(dim * dim_coefficient)\n",
        "        self.linear_v = layers.Dense(dim * dim_coefficient)\n",
        "        self.linear_out = layers.Dense(dim)\n",
        "        self.softmax = layers.Softmax(axis=-1)\n",
        "        self.attention_drop = layers.Dropout(attention_dropout)\n",
        "        self.projection_drop = layers.Dropout(projection_dropout)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        q = self.linear_q(inputs)\n",
        "        k = self.linear_k(inputs)\n",
        "        v = self.linear_v(inputs)\n",
        "\n",
        "        q = tf.reshape(q, (-1, tf.shape(inputs)[1], self.num_heads, self.dim_coefficient))\n",
        "        k = tf.reshape(k, (-1, tf.shape(inputs)[1], self.num_heads, self.dim_coefficient))\n",
        "        v = tf.reshape(v, (-1, tf.shape(inputs)[1], self.num_heads, self.dim_coefficient))\n",
        "\n",
        "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "        k = tf.transpose(k, perm=[0, 2, 1, 3])\n",
        "        v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
        "\n",
        "        attention = self.softmax(tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(float(self.dim_coefficient)))\n",
        "        attention = self.attention_drop(attention, training=training)\n",
        "\n",
        "        out = tf.matmul(attention, v)\n",
        "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (-1, tf.shape(inputs)[1], self.dim * self.dim_coefficient))\n",
        "\n",
        "        out = self.linear_out(out)\n",
        "        out = self.projection_drop(out, training=training)\n",
        "        return out\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          \"dim\": self.dim,\n",
        "          \"num_heads\": self.num_heads,\n",
        "          \"dim_coefficient\": self.dim_coefficient,\n",
        "          \"attention_dropout\": self.attention_drop.rate,\n",
        "          \"projection_dropout\": self.projection_drop.rate\n",
        "      })\n",
        "      return config\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, mlp_dim, drop_rate=0.2, attn_drop=0.2, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = ExternalAttention(\n",
        "            dim=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            attention_dropout=attn_drop,\n",
        "            projection_dropout=drop_rate\n",
        "        )\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(mlp_dim, activation=tf.nn.gelu),\n",
        "            layers.Dropout(drop_rate),\n",
        "            layers.Dense(embedding_dim),\n",
        "            layers.Dropout(drop_rate)\n",
        "        ], name=\"mlp\")\n",
        "\n",
        "        self.dropout1 = layers.Dropout(0.2)\n",
        "        self.dropout2 = layers.Dropout(0.2)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        attn_output = self.attention(self.norm1(x), training=training)\n",
        "        x = x + self.dropout1(attn_output, training=training)\n",
        "        mlp_output = self.mlp(self.norm2(x), training=training)\n",
        "        x = x + self.dropout2(mlp_output, training=training)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embedding_dim\": self.attention.dim,\n",
        "            \"num_heads\": self.attention.num_heads,\n",
        "            \"mlp_dim\": self.mlp.layers[0].units,\n",
        "            \"drop_rate\": self.mlp.layers[1].rate,\n",
        "            \"attn_drop\": self.attention.attention_drop.rate\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def build_hyper_model(hp):\n",
        "    \"\"\"\n",
        "    Builds the EANet model with hyperparameters for tuning.\n",
        "    \"\"\"\n",
        "    #Defining Hyperparameters to Tune\n",
        "    EMBEDDING_DIM = hp.Choice('embedding_dim', [192, 256])\n",
        "    NUM_HEADS = hp.Choice('num_heads', [4, 8])\n",
        "    NUM_TRANSFORMER_BLOCKS = hp.Int('num_blocks', 3, 6)\n",
        "\n",
        "    mlp_dim_factor = hp.Choice('mlp_dim_factor', [2, 4])\n",
        "    MLP_DIM = EMBEDDING_DIM * mlp_dim_factor\n",
        "\n",
        "    #Tuning dropout rates\n",
        "    proj_drop = hp.Float('proj_drop', 0.1, 0.4, step=0.1) # After embedding\n",
        "    transformer_drop_rate = hp.Float('transformer_drop_rate', 0.2, 0.5, step=0.1)\n",
        "    transformer_attn_drop = hp.Float('transformer_attn_drop', 0.2, 0.5, step=0.1)\n",
        "    final_drop = hp.Float('final_dropout', 0.3, 0.6, step=0.1) # Before head\n",
        "\n",
        "    #Model Building\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    inputs = layers.Input(shape=IMG_SIZE + (3,))\n",
        "    patches = PatchExtractor(PATCH_SIZE)(inputs)\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = layers.Reshape((NUM_PATCHES, patch_dims))(patches)\n",
        "    patch_embedding = layers.Dense(units=EMBEDDING_DIM)(patches)\n",
        "\n",
        "    positions = tf.range(start=0, limit=NUM_PATCHES, delta=1)\n",
        "    position_embedding = layers.Embedding(\n",
        "        input_dim=NUM_PATCHES, output_dim=EMBEDDING_DIM\n",
        "    )(positions)\n",
        "\n",
        "    x = patch_embedding + position_embedding\n",
        "    x = layers.Dropout(proj_drop)(x, training=True)\n",
        "\n",
        "    transformer_layers = []\n",
        "    for _ in range(NUM_TRANSFORMER_BLOCKS):\n",
        "        transformer_layers.append(\n",
        "            TransformerBlock(\n",
        "                embedding_dim=EMBEDDING_DIM,\n",
        "                num_heads=NUM_HEADS,\n",
        "                mlp_dim=MLP_DIM,\n",
        "                drop_rate=transformer_drop_rate,\n",
        "                attn_drop=transformer_attn_drop\n",
        "            )\n",
        "        )\n",
        "\n",
        "    transformer_backbone = tf.keras.Sequential(transformer_layers, name=\"transformer_backbone\")\n",
        "    x = transformer_backbone(x)\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(final_drop)(x, training=True)\n",
        "\n",
        "    outputs = layers.Dense(len(CLASSES), activation=\"sigmoid\", dtype='float32')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=\"eanet_hyper_model\")\n",
        "\n",
        "    return model\n",
        "\n",
        "    #Test build\n",
        "    hp = keras_tuner.HyperParameters()\n",
        "    model = build_hyper_model(hp)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vhoiyf7j9BSm"
      },
      "outputs": [],
      "source": [
        "def create_weighted_bce_loss(pos_counts, total_samples, smooth=0.05):\n",
        "    \"\"\"\n",
        "    A factory function that creates a weighted BCE loss function.\n",
        "    \"\"\"\n",
        "    #These are calculated ONCE and captured by the inner function\n",
        "    pos = tf.constant(pos_counts, dtype=tf.float32)\n",
        "    neg = total_samples - pos\n",
        "    w_pos = neg / tf.maximum(pos, 1.0)\n",
        "    w_neg = tf.ones_like(pos)\n",
        "\n",
        "    #This is the actual loss function that will be used\n",
        "    def weighted_bce(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "        #Apply smoothing\n",
        "        y_true = y_true * (1.0 - smooth) + 0.5 * smooth\n",
        "\n",
        "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "        #Apply weights\n",
        "        weights = y_true * w_pos + (1.0 - y_true) * w_neg\n",
        "        return tf.reduce_mean(bce * weights)\n",
        "\n",
        "    return weighted_bce\n",
        "\n",
        "#Creating the loss function instance and custom_objects dict\n",
        "\n",
        "loss_fn = create_weighted_bce_loss(pos_counts, total_train_samples)\n",
        "\n",
        "#Custom Objects needed for loading/saving\n",
        "custom_objects = {\n",
        "    \"PatchExtractor\": PatchExtractor,\n",
        "    \"TransformerBlock\": TransformerBlock,\n",
        "    \"ExternalAttention\": ExternalAttention,\n",
        "    \"weighted_bce\": loss_fn\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UobEDVhaX1e"
      },
      "outputs": [],
      "source": [
        "class CustomTuner(keras_tuner.RandomSearch):\n",
        "    \"\"\"\n",
        "    Custom Tuner to implement the two-stage training logic.\n",
        "    \"\"\"\n",
        "    def __init__(self, loss_function, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def run_trial(self, trial, train_ds, val_ds, **kwargs):\n",
        "        hp = trial.hyperparameters\n",
        "        model = self.hypermodel.build(hp)\n",
        "\n",
        "        #Define ALL Metrics\n",
        "        all_metrics = [\n",
        "            tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
        "            tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n",
        "            tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
        "            tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
        "        ]\n",
        "\n",
        "        #STAGE 1: FEATURE EXTRACTION (Train Head Only)\n",
        "        print(f\"\\n[Trial {trial.trial_id}] Stage 1: Training the classification head...\")\n",
        "        head_lr = hp.Float('head_lr', 1e-4, 1e-3, sampling='log')\n",
        "\n",
        "        model.get_layer(\"transformer_backbone\").trainable = False\n",
        "        model.compile(\n",
        "            optimizer=AdamW(learning_rate=head_lr, weight_decay=1e-4),\n",
        "            loss=self.loss_function,\n",
        "            metrics=all_metrics\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=10,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        #STAGE 2: FINE-TUNING\n",
        "        print(f\"\\n[Trial {trial.trial_id}] Stage 2: Fine-tuning the entire model...\")\n",
        "        finetune_lr = hp.Float('finetune_lr', 1e-6, 5e-5, sampling='log')\n",
        "\n",
        "        model.get_layer(\"transformer_backbone\").trainable = True\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=AdamW(learning_rate=finetune_lr, weight_decay=1e-4),\n",
        "            loss=self.loss_function,\n",
        "            metrics=all_metrics\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_auc\",\n",
        "                mode=\"max\",\n",
        "                patience=5,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_auc\",\n",
        "                mode=\"max\",\n",
        "                factor=0.2,\n",
        "                patience=3\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=70,\n",
        "            callbacks=callbacks,\n",
        "            initial_epoch=10,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        #Report Results\n",
        "        print(f\"[Trial {trial.trial_id}] Evaluating best weights on val_ds...\")\n",
        "        eval_results = model.evaluate(\n",
        "            val_ds,\n",
        "            return_dict=True,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        val_results_with_prefix = {f\"val_{k}\": v for k, v in eval_results.items()}\n",
        "\n",
        "        return val_results_with_prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3DrSsDhXR9D",
        "outputId": "70a52813-ea9e-493c-938d-ba263fe5eb63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 16m 11s]\n",
            "val_auc: 0.9004563689231873\n",
            "\n",
            "Best val_auc So Far: 0.9004563689231873\n",
            "Total elapsed time: 01h 21m 51s\n",
            "\n",
            "Total search time: 81.84 minutes\n"
          ]
        }
      ],
      "source": [
        "#Define the tuner\n",
        "tuner = CustomTuner(\n",
        "    loss_function=loss_fn,\n",
        "    hypermodel=build_hyper_model,\n",
        "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"), #Target metric\n",
        "    max_trials=5,  #How many different models to test\n",
        "    executions_per_trial=1, #How many times to train each model\n",
        "    directory=os.path.join(DATA_ROOT, 'keras_tuner'),\n",
        "    project_name='eanet_skin_tuning',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "#Print a summary of the search space\n",
        "tuner.search_space_summary()\n",
        "\n",
        "#Starting the search\n",
        "print(\"\\nStarting hyperparameter search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "tuner.search(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal search time: {(end_time - start_time) / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2a7ntB6jdYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436c0858-963b-4635-a61f-7b54df933a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Trials\n",
            "Results summary\n",
            "Results in /content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/keras_tuner/eanet_skin_tuning\n",
            "Showing 5 best trials\n",
            "Objective(name=\"val_auc\", direction=\"max\")\n",
            "\n",
            "Trial 4 summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 256\n",
            "num_heads: 8\n",
            "num_blocks: 6\n",
            "mlp_dim_factor: 2\n",
            "proj_drop: 0.1\n",
            "transformer_drop_rate: 0.2\n",
            "transformer_attn_drop: 0.4\n",
            "final_dropout: 0.5\n",
            "head_lr: 0.0001384466273952018\n",
            "finetune_lr: 3.103788942300277e-05\n",
            "Score: 0.9004563689231873\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 256\n",
            "num_heads: 8\n",
            "num_blocks: 4\n",
            "mlp_dim_factor: 4\n",
            "proj_drop: 0.2\n",
            "transformer_drop_rate: 0.2\n",
            "transformer_attn_drop: 0.30000000000000004\n",
            "final_dropout: 0.4\n",
            "head_lr: 0.0001319548679279438\n",
            "finetune_lr: 3.048649562411296e-06\n",
            "Score: 0.8809218406677246\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 192\n",
            "num_heads: 4\n",
            "num_blocks: 6\n",
            "mlp_dim_factor: 4\n",
            "proj_drop: 0.2\n",
            "transformer_drop_rate: 0.4\n",
            "transformer_attn_drop: 0.30000000000000004\n",
            "final_dropout: 0.5\n",
            "head_lr: 0.0004359094441412982\n",
            "finetune_lr: 1.15788289753497e-05\n",
            "Score: 0.877696692943573\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 256\n",
            "num_heads: 4\n",
            "num_blocks: 6\n",
            "mlp_dim_factor: 2\n",
            "proj_drop: 0.1\n",
            "transformer_drop_rate: 0.4\n",
            "transformer_attn_drop: 0.4\n",
            "final_dropout: 0.4\n",
            "head_lr: 0.0001\n",
            "finetune_lr: 1e-06\n",
            "Score: 0.868218183517456\n",
            "\n",
            "Trial 3 summary\n",
            "Hyperparameters:\n",
            "embedding_dim: 256\n",
            "num_heads: 4\n",
            "num_blocks: 3\n",
            "mlp_dim_factor: 2\n",
            "proj_drop: 0.30000000000000004\n",
            "transformer_drop_rate: 0.2\n",
            "transformer_attn_drop: 0.2\n",
            "final_dropout: 0.5\n",
            "head_lr: 0.00016874269759840067\n",
            "finetune_lr: 2.892926258290638e-06\n",
            "Score: 0.8629463315010071\n",
            "\n",
            "Best Hyperparameters Found\n",
            "Embedding Dim: 256\n",
            "Num Blocks: 6\n",
            "Num Heads: 8\n",
            "MLP Factor: 2\n",
            "Proj Dropout: 0.100\n",
            "Transformer Drop: 0.200\n",
            "Transformer Attn Drop: 0.400\n",
            "Final Dropout: 0.500\n",
            "Head LR: 1.4e-04\n",
            "Finetune LR: 3.1e-05\n",
            "\n",
            "Best hyperparameters successfully retrieved.\n"
          ]
        }
      ],
      "source": [
        "# Show the top 3 performing trials\n",
        "print(\"Top 3 Trials\")\n",
        "tuner.results_summary(num_trials=5)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found\")\n",
        "print(f\"Embedding Dim: {best_hps.get('embedding_dim')}\")\n",
        "print(f\"Num Blocks: {best_hps.get('num_blocks')}\")\n",
        "print(f\"Num Heads: {best_hps.get('num_heads')}\")\n",
        "print(f\"MLP Factor: {best_hps.get('mlp_dim_factor')}\")\n",
        "print(f\"Proj Dropout: {best_hps.get('proj_drop'):.3f}\")\n",
        "print(f\"Transformer Drop: {best_hps.get('transformer_drop_rate'):.3f}\")\n",
        "print(f\"Transformer Attn Drop: {best_hps.get('transformer_attn_drop'):.3f}\")\n",
        "print(f\"Final Dropout: {best_hps.get('final_dropout'):.3f}\")\n",
        "print(f\"Head LR: {best_hps.get('head_lr'):.1e}\")\n",
        "print(f\"Finetune LR: {best_hps.get('finetune_lr'):.1e}\")\n",
        "\n",
        "print(\"\\nBest hyperparameters successfully retrieved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the FINAL Best Model\n",
        "print(\"Building the best model with the optimal hyperparameters...\")\n",
        "#best_hps variable comes from running Cell 9\n",
        "final_model = build_hyper_model(best_hps)\n",
        "final_model.summary()\n",
        "\n",
        "#Define Callbacks for FINAL Training\n",
        "#We use the full, original patience settings here\n",
        "FINAL_MODEL_PATH = os.path.join(DATA_ROOT, \"eanet_skin_model_FINAL_TUNED.keras\")\n",
        "final_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=10, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.2, patience=4, min_lr=1e-6),\n",
        "    tf.keras.callbacks.ModelCheckpoint(FINAL_MODEL_PATH, monitor=\"val_auc\", mode=\"max\", save_best_only=True)\n",
        "]\n",
        "\n",
        "#STAGE 1: Train the Head\n",
        "print(\"\\nFinal Training: STAGE 1 (Head)\")\n",
        "#Getting the winning head_lr from best_hps\n",
        "final_head_lr = best_hps.get('head_lr')\n",
        "\n",
        "final_model.get_layer(\"transformer_backbone\").trainable = False\n",
        "final_model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=final_head_lr, weight_decay=1e-4),\n",
        "    loss=loss_fn, #loss_fn from Cell 6\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
        "        tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\")\n",
        "    ]\n",
        ")\n",
        "start_time_stage1 = time.time()\n",
        "history_head = final_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    verbose=1\n",
        ")\n",
        "end_time_stage1 = time.time()\n",
        "\n",
        "#STAGE 2: Fine-Tuning\n",
        "print(\"\\nFinal Training: STAGE 2 (Fine-Tune)\")\n",
        "#Getting the winning finetune_lr from best_hps\n",
        "final_finetune_lr = best_hps.get('finetune_lr')\n",
        "\n",
        "final_model.get_layer(\"transformer_backbone\").trainable = True\n",
        "final_model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=final_finetune_lr, weight_decay=1e-4),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
        "        tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\")\n",
        "    ]\n",
        ")\n",
        "start_time_stage2 = time.time()\n",
        "history_fine_tune = final_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=70,\n",
        "    callbacks=final_callbacks,\n",
        "    initial_epoch=len(history_head.history['loss']),\n",
        "    verbose=1\n",
        ")\n",
        "end_time_stage2 = time.time()\n",
        "\n",
        "#Final Evaluation on Test Set\n",
        "#The ModelCheckpoint callback will have saved the best model.\n",
        "#We load it back to ensure we evaluate the *very best* version.\n",
        "print(f\"\\nLoading best saved final model from: {FINAL_MODEL_PATH}\")\n",
        "loaded_best_model = tf.keras.models.load_model(\n",
        "    FINAL_MODEL_PATH,\n",
        "    custom_objects=custom_objects\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluating the final tuned model on the test set...\")\n",
        "test_results = loaded_best_model.evaluate(test_ds, return_dict=True)\n",
        "\n",
        "print(\"\\nFinal Test Set Evaluation Results\")\n",
        "precision = 0.0\n",
        "recall = 0.0\n",
        "for metric, value in test_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "    if metric == \"precision\":\n",
        "        precision = value\n",
        "    if metric == \"recall\":\n",
        "        recall = value\n",
        "\n",
        "#Manually calculate F1 Score\n",
        "if precision + recall > 0:\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print(f\"f1_score (calculated): {f1_score:.4f}\")\n",
        "else:\n",
        "    print(\"f1_score (calculated): 0.0\")\n",
        "\n",
        "#Print Stats\n",
        "print(\"\\nFinal Model Stats\")\n",
        "if os.path.exists(FINAL_MODEL_PATH):\n",
        "    file_size_bytes = os.path.getsize(FINAL_MODEL_PATH)\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "    print(f\"Model Size on Disk: {file_size_mb:.2f} MB\")\n",
        "\n",
        "total_time_sec = (end_time_stage1 - start_time_stage1) + (end_time_stage2 - start_time_stage2)\n",
        "total_epochs_ran = len(history_head.history['loss']) + len(history_fine_tune.history['loss'])\n",
        "avg_time_per_epoch_sec = total_time_sec / total_epochs_ran\n",
        "print(f\"Total Training Time: {total_time_sec / 60:.2f} minutes\")\n",
        "print(f\"Total Epochs Trained: {total_epochs_ran}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IjCP3O6_Yl5f",
        "outputId": "2eb9ffb1-b9ce-441b-dc20-eaa9e7d70b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the best model with the optimal hyperparameters...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"eanet_hyper_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"eanet_hyper_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_extractor                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m3072\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mPatchExtractor\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m3072\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m786,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_backbone            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m7,895,040\u001b[0m │\n",
              "│ (\u001b[38;5;33mSequential\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m771\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_extractor                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchExtractor</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">786,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_backbone            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,895,040</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,683,011\u001b[0m (33.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,683,011</span> (33.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,683,011\u001b[0m (33.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,683,011</span> (33.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Training: STAGE 1 (Head)\n",
            "Epoch 1/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 425ms/step - acc: 0.5686 - auc: 0.5545 - loss: 1.3060 - precision: 0.2242 - recall: 0.4985 - val_acc: 0.7208 - val_auc: 0.8022 - val_loss: 1.0490 - val_precision: 0.3918 - val_recall: 0.7804\n",
            "Epoch 2/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 149ms/step - acc: 0.6601 - auc: 0.6864 - loss: 1.1154 - precision: 0.3059 - recall: 0.5941 - val_acc: 0.7183 - val_auc: 0.8123 - val_loss: 1.0976 - val_precision: 0.3901 - val_recall: 0.7884\n",
            "Epoch 3/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - acc: 0.6902 - auc: 0.7215 - loss: 1.0725 - precision: 0.3412 - recall: 0.6430 - val_acc: 0.7678 - val_auc: 0.8155 - val_loss: 1.0220 - val_precision: 0.4430 - val_recall: 0.7407\n",
            "Epoch 4/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 148ms/step - acc: 0.7028 - auc: 0.7414 - loss: 1.0381 - precision: 0.3552 - recall: 0.6499 - val_acc: 0.7399 - val_auc: 0.8289 - val_loss: 0.9941 - val_precision: 0.4153 - val_recall: 0.8175\n",
            "Epoch 5/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 151ms/step - acc: 0.7020 - auc: 0.7517 - loss: 1.0187 - precision: 0.3558 - recall: 0.6639 - val_acc: 0.7332 - val_auc: 0.8301 - val_loss: 1.0622 - val_precision: 0.4077 - val_recall: 0.8122\n",
            "Epoch 6/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 154ms/step - acc: 0.7087 - auc: 0.7554 - loss: 1.0218 - precision: 0.3656 - recall: 0.6842 - val_acc: 0.7487 - val_auc: 0.8317 - val_loss: 0.9795 - val_precision: 0.4240 - val_recall: 0.8042\n",
            "Epoch 7/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 160ms/step - acc: 0.7259 - auc: 0.7669 - loss: 1.0004 - precision: 0.3852 - recall: 0.6961 - val_acc: 0.7348 - val_auc: 0.8396 - val_loss: 0.9892 - val_precision: 0.4110 - val_recall: 0.8307\n",
            "Epoch 8/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - acc: 0.7240 - auc: 0.7677 - loss: 1.0028 - precision: 0.3830 - recall: 0.6936 - val_acc: 0.7327 - val_auc: 0.8422 - val_loss: 0.9846 - val_precision: 0.4093 - val_recall: 0.8360\n",
            "Epoch 9/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 176ms/step - acc: 0.7271 - auc: 0.7782 - loss: 0.9841 - precision: 0.3881 - recall: 0.7059 - val_acc: 0.7441 - val_auc: 0.8452 - val_loss: 0.9767 - val_precision: 0.4211 - val_recall: 0.8333\n",
            "Epoch 10/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 170ms/step - acc: 0.7276 - auc: 0.7784 - loss: 0.9858 - precision: 0.3890 - recall: 0.7105 - val_acc: 0.7580 - val_auc: 0.8481 - val_loss: 0.9583 - val_precision: 0.4355 - val_recall: 0.8122\n",
            "Epoch 11/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 178ms/step - acc: 0.7375 - auc: 0.7813 - loss: 0.9826 - precision: 0.3992 - recall: 0.7028 - val_acc: 0.7508 - val_auc: 0.8495 - val_loss: 0.9646 - val_precision: 0.4278 - val_recall: 0.8228\n",
            "Epoch 12/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 171ms/step - acc: 0.7398 - auc: 0.7924 - loss: 0.9638 - precision: 0.4025 - recall: 0.7086 - val_acc: 0.7626 - val_auc: 0.8499 - val_loss: 0.9637 - val_precision: 0.4409 - val_recall: 0.8095\n",
            "Epoch 13/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 179ms/step - acc: 0.7430 - auc: 0.7942 - loss: 0.9613 - precision: 0.4070 - recall: 0.7134 - val_acc: 0.7673 - val_auc: 0.8517 - val_loss: 0.9355 - val_precision: 0.4469 - val_recall: 0.8122\n",
            "Epoch 14/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 171ms/step - acc: 0.7465 - auc: 0.7941 - loss: 0.9554 - precision: 0.4111 - recall: 0.7133 - val_acc: 0.7482 - val_auc: 0.8557 - val_loss: 0.9768 - val_precision: 0.4247 - val_recall: 0.8201\n",
            "Epoch 15/15\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 180ms/step - acc: 0.7407 - auc: 0.7971 - loss: 0.9559 - precision: 0.4051 - recall: 0.7217 - val_acc: 0.7580 - val_auc: 0.8553 - val_loss: 0.9551 - val_precision: 0.4353 - val_recall: 0.8095\n",
            "\n",
            "Final Training: STAGE 2 (Fine-Tune)\n",
            "Epoch 16/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 543ms/step - acc: 0.7500 - auc: 0.8085 - loss: 0.9325 - precision: 0.4180 - recall: 0.7398 - val_acc: 0.7456 - val_auc: 0.8653 - val_loss: 0.9274 - val_precision: 0.4246 - val_recall: 0.8571 - learning_rate: 3.1038e-05\n",
            "Epoch 17/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 209ms/step - acc: 0.7646 - auc: 0.8221 - loss: 0.9072 - precision: 0.4360 - recall: 0.7340 - val_acc: 0.7482 - val_auc: 0.8725 - val_loss: 0.9099 - val_precision: 0.4272 - val_recall: 0.8545 - learning_rate: 3.1038e-05\n",
            "Epoch 18/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 226ms/step - acc: 0.7817 - auc: 0.8412 - loss: 0.8741 - precision: 0.4617 - recall: 0.7609 - val_acc: 0.7250 - val_auc: 0.8737 - val_loss: 0.9327 - val_precision: 0.4054 - val_recall: 0.8783 - learning_rate: 3.1038e-05\n",
            "Epoch 19/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 226ms/step - acc: 0.7841 - auc: 0.8431 - loss: 0.8687 - precision: 0.4655 - recall: 0.7709 - val_acc: 0.7281 - val_auc: 0.8787 - val_loss: 0.9372 - val_precision: 0.4088 - val_recall: 0.8836 - learning_rate: 3.1038e-05\n",
            "Epoch 20/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 217ms/step - acc: 0.7805 - auc: 0.8508 - loss: 0.8503 - precision: 0.4613 - recall: 0.7854 - val_acc: 0.7322 - val_auc: 0.8820 - val_loss: 0.9200 - val_precision: 0.4135 - val_recall: 0.8915 - learning_rate: 3.1038e-05\n",
            "Epoch 21/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 205ms/step - acc: 0.7911 - auc: 0.8550 - loss: 0.8416 - precision: 0.4759 - recall: 0.7730 - val_acc: 0.7317 - val_auc: 0.8807 - val_loss: 0.9309 - val_precision: 0.4121 - val_recall: 0.8810 - learning_rate: 3.1038e-05\n",
            "Epoch 22/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 192ms/step - acc: 0.7918 - auc: 0.8596 - loss: 0.8315 - precision: 0.4771 - recall: 0.7749 - val_acc: 0.7317 - val_auc: 0.8835 - val_loss: 0.9419 - val_precision: 0.4123 - val_recall: 0.8836 - learning_rate: 3.1038e-05\n",
            "Epoch 23/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 213ms/step - acc: 0.7954 - auc: 0.8625 - loss: 0.8239 - precision: 0.4826 - recall: 0.7793 - val_acc: 0.7270 - val_auc: 0.8856 - val_loss: 0.9243 - val_precision: 0.4096 - val_recall: 0.9048 - learning_rate: 3.1038e-05\n",
            "Epoch 24/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 226ms/step - acc: 0.8013 - auc: 0.8673 - loss: 0.8166 - precision: 0.4919 - recall: 0.7873 - val_acc: 0.7348 - val_auc: 0.8880 - val_loss: 0.9273 - val_precision: 0.4158 - val_recall: 0.8889 - learning_rate: 3.1038e-05\n",
            "Epoch 25/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 207ms/step - acc: 0.8051 - auc: 0.8713 - loss: 0.8038 - precision: 0.4982 - recall: 0.7966 - val_acc: 0.7276 - val_auc: 0.8891 - val_loss: 0.9422 - val_precision: 0.4085 - val_recall: 0.8862 - learning_rate: 3.1038e-05\n",
            "Epoch 26/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 211ms/step - acc: 0.8045 - auc: 0.8765 - loss: 0.7919 - precision: 0.4971 - recall: 0.8091 - val_acc: 0.7389 - val_auc: 0.8915 - val_loss: 0.9381 - val_precision: 0.4184 - val_recall: 0.8677 - learning_rate: 3.1038e-05\n",
            "Epoch 27/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 212ms/step - acc: 0.8078 - auc: 0.8780 - loss: 0.7884 - precision: 0.5025 - recall: 0.8050 - val_acc: 0.7301 - val_auc: 0.8939 - val_loss: 0.9363 - val_precision: 0.4110 - val_recall: 0.8862 - learning_rate: 3.1038e-05\n",
            "Epoch 28/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 208ms/step - acc: 0.8117 - auc: 0.8814 - loss: 0.7793 - precision: 0.5089 - recall: 0.8046 - val_acc: 0.7358 - val_auc: 0.8949 - val_loss: 0.9452 - val_precision: 0.4150 - val_recall: 0.8651 - learning_rate: 3.1038e-05\n",
            "Epoch 29/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 211ms/step - acc: 0.8127 - auc: 0.8843 - loss: 0.7680 - precision: 0.5104 - recall: 0.8061 - val_acc: 0.7307 - val_auc: 0.8955 - val_loss: 0.9416 - val_precision: 0.4107 - val_recall: 0.8757 - learning_rate: 3.1038e-05\n",
            "Epoch 30/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 209ms/step - acc: 0.8122 - auc: 0.8833 - loss: 0.7724 - precision: 0.5097 - recall: 0.8021 - val_acc: 0.7322 - val_auc: 0.8969 - val_loss: 0.9430 - val_precision: 0.4120 - val_recall: 0.8730 - learning_rate: 3.1038e-05\n",
            "Epoch 31/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 204ms/step - acc: 0.8207 - auc: 0.8949 - loss: 0.7477 - precision: 0.5237 - recall: 0.8216 - val_acc: 0.7337 - val_auc: 0.8989 - val_loss: 0.9457 - val_precision: 0.4137 - val_recall: 0.8757 - learning_rate: 3.1038e-05\n",
            "Epoch 32/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 210ms/step - acc: 0.8219 - auc: 0.8959 - loss: 0.7424 - precision: 0.5258 - recall: 0.8214 - val_acc: 0.7353 - val_auc: 0.8993 - val_loss: 0.9460 - val_precision: 0.4149 - val_recall: 0.8704 - learning_rate: 3.1038e-05\n",
            "Epoch 33/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 217ms/step - acc: 0.8241 - auc: 0.8959 - loss: 0.7411 - precision: 0.5295 - recall: 0.8216 - val_acc: 0.7296 - val_auc: 0.9015 - val_loss: 0.9288 - val_precision: 0.4101 - val_recall: 0.8810 - learning_rate: 3.1038e-05\n",
            "Epoch 34/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 201ms/step - acc: 0.8239 - auc: 0.8987 - loss: 0.7330 - precision: 0.5294 - recall: 0.8179 - val_acc: 0.7239 - val_auc: 0.8985 - val_loss: 0.9456 - val_precision: 0.4042 - val_recall: 0.8757 - learning_rate: 3.1038e-05\n",
            "Epoch 35/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 175ms/step - acc: 0.8241 - auc: 0.8974 - loss: 0.7403 - precision: 0.5300 - recall: 0.8098 - val_acc: 0.7503 - val_auc: 0.9045 - val_loss: 0.8912 - val_precision: 0.4308 - val_recall: 0.8730 - learning_rate: 3.1038e-05\n",
            "Epoch 36/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 197ms/step - acc: 0.8328 - auc: 0.9028 - loss: 0.7248 - precision: 0.5454 - recall: 0.8207 - val_acc: 0.7492 - val_auc: 0.9011 - val_loss: 0.9059 - val_precision: 0.4280 - val_recall: 0.8492 - learning_rate: 3.1038e-05\n",
            "Epoch 37/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 177ms/step - acc: 0.8349 - auc: 0.9065 - loss: 0.7148 - precision: 0.5486 - recall: 0.8325 - val_acc: 0.7477 - val_auc: 0.9056 - val_loss: 0.8958 - val_precision: 0.4275 - val_recall: 0.8651 - learning_rate: 3.1038e-05\n",
            "Epoch 38/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 206ms/step - acc: 0.8347 - auc: 0.9063 - loss: 0.7137 - precision: 0.5481 - recall: 0.8338 - val_acc: 0.7523 - val_auc: 0.9073 - val_loss: 0.8868 - val_precision: 0.4322 - val_recall: 0.8598 - learning_rate: 3.1038e-05\n",
            "Epoch 39/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 196ms/step - acc: 0.8386 - auc: 0.9094 - loss: 0.7070 - precision: 0.5547 - recall: 0.8455 - val_acc: 0.7657 - val_auc: 0.9066 - val_loss: 0.8957 - val_precision: 0.4462 - val_recall: 0.8333 - learning_rate: 3.1038e-05\n",
            "Epoch 40/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 186ms/step - acc: 0.8367 - auc: 0.9084 - loss: 0.7078 - precision: 0.5520 - recall: 0.8323 - val_acc: 0.7647 - val_auc: 0.9088 - val_loss: 0.8858 - val_precision: 0.4449 - val_recall: 0.8333 - learning_rate: 3.1038e-05\n",
            "Epoch 41/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 201ms/step - acc: 0.8410 - auc: 0.9108 - loss: 0.7034 - precision: 0.5594 - recall: 0.8437 - val_acc: 0.7632 - val_auc: 0.9062 - val_loss: 0.8838 - val_precision: 0.4424 - val_recall: 0.8228 - learning_rate: 3.1038e-05\n",
            "Epoch 42/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 179ms/step - acc: 0.8423 - auc: 0.9159 - loss: 0.6871 - precision: 0.5616 - recall: 0.8477 - val_acc: 0.7652 - val_auc: 0.9082 - val_loss: 0.8656 - val_precision: 0.4449 - val_recall: 0.8228 - learning_rate: 3.1038e-05\n",
            "Epoch 43/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 177ms/step - acc: 0.8449 - auc: 0.9147 - loss: 0.6920 - precision: 0.5666 - recall: 0.8464 - val_acc: 0.7848 - val_auc: 0.9098 - val_loss: 0.8821 - val_precision: 0.4698 - val_recall: 0.8016 - learning_rate: 3.1038e-05\n",
            "Epoch 44/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 210ms/step - acc: 0.8466 - auc: 0.9167 - loss: 0.6822 - precision: 0.5706 - recall: 0.8413 - val_acc: 0.7822 - val_auc: 0.9145 - val_loss: 0.8706 - val_precision: 0.4664 - val_recall: 0.8069 - learning_rate: 3.1038e-05\n",
            "Epoch 45/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 198ms/step - acc: 0.8514 - auc: 0.9200 - loss: 0.6759 - precision: 0.5793 - recall: 0.8513 - val_acc: 0.7884 - val_auc: 0.9127 - val_loss: 0.8528 - val_precision: 0.4752 - val_recall: 0.8122 - learning_rate: 3.1038e-05\n",
            "Epoch 46/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 180ms/step - acc: 0.8489 - auc: 0.9176 - loss: 0.6829 - precision: 0.5745 - recall: 0.8481 - val_acc: 0.7730 - val_auc: 0.9147 - val_loss: 0.8995 - val_precision: 0.4530 - val_recall: 0.7910 - learning_rate: 3.1038e-05\n",
            "Epoch 47/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 192ms/step - acc: 0.8514 - auc: 0.9202 - loss: 0.6757 - precision: 0.5781 - recall: 0.8601 - val_acc: 0.7859 - val_auc: 0.9128 - val_loss: 0.8797 - val_precision: 0.4707 - val_recall: 0.7857 - learning_rate: 3.1038e-05\n",
            "Epoch 48/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 178ms/step - acc: 0.8546 - auc: 0.9256 - loss: 0.6607 - precision: 0.5849 - recall: 0.8572 - val_acc: 0.7936 - val_auc: 0.9160 - val_loss: 0.8722 - val_precision: 0.4820 - val_recall: 0.7804 - learning_rate: 3.1038e-05\n",
            "Epoch 49/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 193ms/step - acc: 0.8567 - auc: 0.9259 - loss: 0.6574 - precision: 0.5902 - recall: 0.8536 - val_acc: 0.7926 - val_auc: 0.9146 - val_loss: 0.8952 - val_precision: 0.4801 - val_recall: 0.7646 - learning_rate: 3.1038e-05\n",
            "Epoch 50/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - acc: 0.8557 - auc: 0.9290 - loss: 0.6483 - precision: 0.5867 - recall: 0.8642 - val_acc: 0.7946 - val_auc: 0.9149 - val_loss: 0.8989 - val_precision: 0.4832 - val_recall: 0.7593 - learning_rate: 3.1038e-05\n",
            "Epoch 51/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 164ms/step - acc: 0.8608 - auc: 0.9287 - loss: 0.6479 - precision: 0.5973 - recall: 0.8636 - val_acc: 0.7771 - val_auc: 0.9081 - val_loss: 0.9463 - val_precision: 0.4551 - val_recall: 0.7249 - learning_rate: 3.1038e-05\n",
            "Epoch 52/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 178ms/step - acc: 0.8610 - auc: 0.9303 - loss: 0.6452 - precision: 0.5972 - recall: 0.8689 - val_acc: 0.8070 - val_auc: 0.9165 - val_loss: 0.8963 - val_precision: 0.5036 - val_recall: 0.7460 - learning_rate: 3.1038e-05\n",
            "Epoch 53/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 193ms/step - acc: 0.8640 - auc: 0.9340 - loss: 0.6312 - precision: 0.6031 - recall: 0.8717 - val_acc: 0.8065 - val_auc: 0.9142 - val_loss: 0.9228 - val_precision: 0.5028 - val_recall: 0.7169 - learning_rate: 3.1038e-05\n",
            "Epoch 54/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 175ms/step - acc: 0.8708 - auc: 0.9354 - loss: 0.6265 - precision: 0.6194 - recall: 0.8675 - val_acc: 0.8096 - val_auc: 0.9192 - val_loss: 0.8819 - val_precision: 0.5083 - val_recall: 0.7328 - learning_rate: 3.1038e-05\n",
            "Epoch 55/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 203ms/step - acc: 0.8744 - auc: 0.9362 - loss: 0.6235 - precision: 0.6266 - recall: 0.8724 - val_acc: 0.8101 - val_auc: 0.9195 - val_loss: 0.9468 - val_precision: 0.5096 - val_recall: 0.7011 - learning_rate: 3.1038e-05\n",
            "Epoch 56/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 212ms/step - acc: 0.8705 - auc: 0.9378 - loss: 0.6189 - precision: 0.6181 - recall: 0.8703 - val_acc: 0.8003 - val_auc: 0.9146 - val_loss: 0.9774 - val_precision: 0.4916 - val_recall: 0.6958 - learning_rate: 3.1038e-05\n",
            "Epoch 57/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - acc: 0.8678 - auc: 0.9366 - loss: 0.6227 - precision: 0.6126 - recall: 0.8660 - val_acc: 0.8044 - val_auc: 0.9124 - val_loss: 1.0250 - val_precision: 0.4990 - val_recall: 0.6534 - learning_rate: 3.1038e-05\n",
            "Epoch 58/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 168ms/step - acc: 0.8737 - auc: 0.9360 - loss: 0.6225 - precision: 0.6238 - recall: 0.8804 - val_acc: 0.8013 - val_auc: 0.9137 - val_loss: 0.9879 - val_precision: 0.4932 - val_recall: 0.6693 - learning_rate: 3.1038e-05\n",
            "Epoch 59/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - acc: 0.8736 - auc: 0.9406 - loss: 0.6052 - precision: 0.6249 - recall: 0.8733 - val_acc: 0.8075 - val_auc: 0.9165 - val_loss: 0.9798 - val_precision: 0.5049 - val_recall: 0.6878 - learning_rate: 3.1038e-05\n",
            "Epoch 60/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 193ms/step - acc: 0.8806 - auc: 0.9467 - loss: 0.5869 - precision: 0.6379 - recall: 0.8877 - val_acc: 0.8189 - val_auc: 0.9258 - val_loss: 0.9040 - val_precision: 0.5254 - val_recall: 0.7381 - learning_rate: 6.2076e-06\n",
            "Epoch 61/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 195ms/step - acc: 0.8834 - auc: 0.9510 - loss: 0.5700 - precision: 0.6441 - recall: 0.8896 - val_acc: 0.8194 - val_auc: 0.9256 - val_loss: 0.9168 - val_precision: 0.5268 - val_recall: 0.7275 - learning_rate: 6.2076e-06\n",
            "Epoch 62/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 200ms/step - acc: 0.8827 - auc: 0.9518 - loss: 0.5658 - precision: 0.6433 - recall: 0.8859 - val_acc: 0.8168 - val_auc: 0.9263 - val_loss: 0.9308 - val_precision: 0.5220 - val_recall: 0.7222 - learning_rate: 6.2076e-06\n",
            "Epoch 63/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 192ms/step - acc: 0.8869 - auc: 0.9519 - loss: 0.5674 - precision: 0.6530 - recall: 0.8880 - val_acc: 0.8189 - val_auc: 0.9260 - val_loss: 0.9376 - val_precision: 0.5262 - val_recall: 0.7169 - learning_rate: 6.2076e-06\n",
            "Epoch 64/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - acc: 0.8840 - auc: 0.9517 - loss: 0.5655 - precision: 0.6457 - recall: 0.8891 - val_acc: 0.8173 - val_auc: 0.9235 - val_loss: 0.9442 - val_precision: 0.5236 - val_recall: 0.7037 - learning_rate: 6.2076e-06\n",
            "Epoch 65/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - acc: 0.8845 - auc: 0.9511 - loss: 0.5705 - precision: 0.6468 - recall: 0.8895 - val_acc: 0.8179 - val_auc: 0.9250 - val_loss: 0.9314 - val_precision: 0.5245 - val_recall: 0.7090 - learning_rate: 6.2076e-06\n",
            "Epoch 66/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - acc: 0.8891 - auc: 0.9536 - loss: 0.5581 - precision: 0.6570 - recall: 0.8941 - val_acc: 0.8189 - val_auc: 0.9252 - val_loss: 0.9671 - val_precision: 0.5267 - val_recall: 0.7037 - learning_rate: 6.2076e-06\n",
            "Epoch 67/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 179ms/step - acc: 0.8853 - auc: 0.9516 - loss: 0.5656 - precision: 0.6487 - recall: 0.8890 - val_acc: 0.8230 - val_auc: 0.9284 - val_loss: 0.9690 - val_precision: 0.5355 - val_recall: 0.6984 - learning_rate: 1.2415e-06\n",
            "Epoch 68/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 229ms/step - acc: 0.8857 - auc: 0.9534 - loss: 0.5613 - precision: 0.6507 - recall: 0.8846 - val_acc: 0.8215 - val_auc: 0.9284 - val_loss: 0.9675 - val_precision: 0.5321 - val_recall: 0.7011 - learning_rate: 1.2415e-06\n",
            "Epoch 69/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 230ms/step - acc: 0.8899 - auc: 0.9565 - loss: 0.5501 - precision: 0.6597 - recall: 0.8924 - val_acc: 0.8240 - val_auc: 0.9295 - val_loss: 0.9694 - val_precision: 0.5374 - val_recall: 0.7037 - learning_rate: 1.2415e-06\n",
            "Epoch 70/70\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 222ms/step - acc: 0.8889 - auc: 0.9532 - loss: 0.5583 - precision: 0.6572 - recall: 0.8928 - val_acc: 0.8225 - val_auc: 0.9298 - val_loss: 0.9779 - val_precision: 0.5347 - val_recall: 0.6931 - learning_rate: 1.2415e-06\n",
            "\n",
            "Loading best saved final model from: /content/drive/MyDrive/skincareapp/acne clean pigmentation wrinkles/eanet_skin_model_FINAL_TUNED.keras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the final tuned model on the test set...\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 13s/step - acc: 0.8423 - auc: 0.9191 - loss: 0.9537 - precision: 0.5638 - recall: 0.7305\n",
            "\n",
            "Final Test Set Evaluation Results\n",
            "acc: 0.8329\n",
            "auc: 0.9216\n",
            "loss: 0.9914\n",
            "precision: 0.5565\n",
            "recall: 0.7079\n",
            "f1_score (calculated): 0.6231\n",
            "\n",
            "Final Model Stats\n",
            "Model Size on Disk: 99.98 MB\n",
            "Total Training Time: 28.26 minutes\n",
            "Total Epochs Trained: 70\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}