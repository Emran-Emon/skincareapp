{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: Setup and Imports\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, random\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Sidenote: Mount Google Drive to access your dataset and save models.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuOKNFd9SBWK",
        "outputId": "c19d9d88-fa76-4c0c-99f1-8f6c59e48610"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: Configuration and Seed\n",
        "# ==============================================================================\n",
        "# Sidenote: A function to set random seeds for reproducibility.\n",
        "def set_seeds(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "# --- Configuration ---\n",
        "IMG_SIZE = (224, 224)  # Sidenote: All images will be resized to this dimension.\n",
        "BATCH_SIZE = 32        # Sidenote: Number of images the model processes at one time.\n",
        "CLASSES = [\"acne\", \"pigmentation\", \"wrinkles\"]  # Sidenote: Target labels for multi-label classification.\n",
        "DATA_ROOT = \"/content/drive/MyDrive/acne clean pigmentation wrinkles/\"  # (paths kept the same)\n",
        "\n",
        "# --- Load CSV and prepare file paths ---\n",
        "df = pd.read_csv(os.path.join(DATA_ROOT, \"labels.csv\"))\n",
        "df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.join(DATA_ROOT, x))\n"
      ],
      "metadata": {
        "id": "WOxOGLuUSChT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: Data Splitting (Train, Validation, Test)\n",
        "# ==============================================================================\n",
        "# Sidenote: Stratified multi-label split.\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=df[CLASSES]\n",
        ")\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=train_val_df[CLASSES]\n",
        ")\n",
        "\n",
        "# --- Class counts for weighted loss ---\n",
        "pos_counts = train_df[CLASSES].sum().values.astype(\"float32\")\n",
        "neg_counts = (len(train_df) - pos_counts).astype(\"float32\")\n",
        "\n",
        "print(\"Train size:\", len(train_df), \"| Val size:\", len(val_df), \"| Test size:\", len(test_df))\n",
        "print(\"Pos counts:\", pos_counts, \"| Neg counts:\", neg_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WC8GFs6SOl-",
        "outputId": "ef5ef4d9-3ccb-4100-b78f-0013c552d2cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 3656 | Val size: 646 | Test size: 760\n",
            "Pos counts: [1015.  386.  738.] | Neg counts: [2641. 3270. 2918.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 4: Create tf.data Pipelines\n",
        "# ==============================================================================\n",
        "def parse_function(filename, labels):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image_decoded = tf.io.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
        "    image_resized = tf.image.resize(image, IMG_SIZE)\n",
        "    return image_resized, labels\n",
        "\n",
        "def create_dataset(df, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (df[\"filename\"].values, df[CLASSES].values.astype(np.float32))\n",
        "    )\n",
        "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_ds = create_dataset(train_df, BATCH_SIZE)\n",
        "val_ds   = create_dataset(val_df,   BATCH_SIZE)\n",
        "test_ds  = create_dataset(test_df,  BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "TdkYM09-STxU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 5: MobileViT Model Implementation (TensorFlow/Keras)\n",
        "# ==============================================================================\n",
        "# This is a compact MobileViT-style implementation:\n",
        "# 1) Local convs\n",
        "# 2) Unfold (non-overlapping patches) -> Transformer (global) -> Fold back\n",
        "# 3) Fuse local + global features\n",
        "\n",
        "class MobileViTBlock(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,                 # transformer channel dimension\n",
        "        patch_size=2,        # non-overlapping unfolding patch size over feature map\n",
        "        depth=2,             # number of transformer blocks\n",
        "        num_heads=4,\n",
        "        mlp_ratio=2.0,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop=0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "        self.depth = depth\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.drop_rate = drop_rate\n",
        "        self.attn_drop = attn_drop\n",
        "\n",
        "        # Local representation (lightweight convs)\n",
        "        self.local_conv1 = layers.Conv2D(dim, 3, padding=\"same\", activation=\"swish\")\n",
        "        self.local_conv2 = layers.Conv2D(dim, 1, padding=\"same\", activation=\"swish\")\n",
        "\n",
        "        # Transformer parts (stacked)\n",
        "        self.norms1 = [layers.LayerNormalization(epsilon=1e-6) for _ in range(depth)]\n",
        "        self.attns  = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim, dropout=attn_drop)\n",
        "                       for _ in range(depth)]\n",
        "        self.dropouts = [layers.Dropout(drop_rate) for _ in range(depth)]\n",
        "        self.norms2 = [layers.LayerNormalization(epsilon=1e-6) for _ in range(depth)]\n",
        "        self.mlps   = []\n",
        "        for _ in range(depth):\n",
        "            self.mlps.append(tf.keras.Sequential([\n",
        "                layers.Dense(int(dim * mlp_ratio), activation=\"swish\"),\n",
        "                layers.Dropout(drop_rate),\n",
        "                layers.Dense(dim),\n",
        "                layers.Dropout(drop_rate)\n",
        "            ]))\n",
        "\n",
        "        # Projection to/from patch-emb dims\n",
        "        self.proj_in  = layers.Dense(dim)\n",
        "        self.proj_out = None  # initialized in build() since it depends on C and patch_size\n",
        "\n",
        "        # Fusion after folding back\n",
        "        self.fuse_conv = layers.Conv2D(dim, 1, padding=\"same\", activation=\"swish\")\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # input: (B, H, W, C)\n",
        "        _, H, W, C = input_shape\n",
        "        p = self.patch_size\n",
        "        # the unfolded patch feature dim is p*p*C\n",
        "        self.patch_feat_dim = p * p * C\n",
        "        self.proj_out = layers.Dense(self.patch_feat_dim)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def _unfold(self, x):\n",
        "        # x: (B, H, W, C)\n",
        "        p = self.patch_size\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=x,\n",
        "            sizes=[1, p, p, 1],\n",
        "            strides=[1, p, p, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        # patches: (B, H//p, W//p, p*p*C)\n",
        "        B = tf.shape(x)[0]\n",
        "        Ph = tf.shape(patches)[1]\n",
        "        Pw = tf.shape(patches)[2]\n",
        "        D  = tf.shape(patches)[3]  # p*p*C\n",
        "        seq = tf.reshape(patches, (B, Ph * Pw, D))  # (B, N, Pdim)\n",
        "        return seq, (Ph, Pw)\n",
        "\n",
        "    def _fold(self, seq, grid_hw, x_hw_c):\n",
        "        # seq: (B, N, Pdim), where Pdim = p*p*C\n",
        "        # grid_hw: (Ph, Pw)\n",
        "        # x_hw_c: (H, W, C) for the original feature shape\n",
        "        B = tf.shape(seq)[0]\n",
        "        Ph, Pw = grid_hw\n",
        "        H, W, C = x_hw_c\n",
        "        p = self.patch_size\n",
        "        # back to (B, Ph, Pw, p*p*C)\n",
        "        patches = tf.reshape(seq, (B, Ph, Pw, p * p * C))\n",
        "        # fold back to (B, H, W, C)\n",
        "        x = tf.reshape(patches, (B, Ph, Pw, p, p, C))\n",
        "        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])  # (B, Ph, p, Pw, p, C)\n",
        "        x = tf.reshape(x, (B, H, W, C))\n",
        "        return x\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # x: (B, H, W, C)\n",
        "        H = tf.shape(x)[1]\n",
        "        W = tf.shape(x)[2]\n",
        "        C = tf.shape(x)[3]\n",
        "        p = self.patch_size\n",
        "\n",
        "        # 1) Local representation\n",
        "        y_local = self.local_conv1(x)\n",
        "        y_local = self.local_conv2(y_local)\n",
        "\n",
        "        # 2) Global representation via unfold -> transformer -> fold\n",
        "        seq, (Ph, Pw) = self._unfold(y_local)                       # (B, N, p*p*C)\n",
        "        seq = self.proj_in(seq)                                     # (B, N, dim)\n",
        "        for ln1, attn, drop, ln2, mlp in zip(self.norms1, self.attns, self.dropouts, self.norms2, self.mlps):\n",
        "            # Transformer block\n",
        "            z = ln1(seq)\n",
        "            z = attn(z, z, training=training)\n",
        "            z = drop(z, training=training)\n",
        "            seq = seq + z\n",
        "            z = ln2(seq)\n",
        "            z = mlp(z, training=training)\n",
        "            seq = seq + z\n",
        "\n",
        "        seq = self.proj_out(seq)                                    # (B, N, p*p*C)\n",
        "        y_global = self._fold(seq, (Ph, Pw), (H, W, C))             # (B, H, W, C)\n",
        "\n",
        "        # 3) Fusion (concat + 1x1)\n",
        "        y = tf.concat([x, y_local, y_global], axis=-1)\n",
        "        y = self.fuse_conv(y)\n",
        "        return y\n",
        "\n",
        "def build_mobilevit_backbone(input_tensor, variant=\"s\"):\n",
        "    # A tiny MobileViT-style backbone with downsampling stems + three MVIT blocks.\n",
        "    # You can tweak channels/patch_sizes/depths for XXS/XS/S variants.\n",
        "    if variant == \"xxs\":\n",
        "        dims = [48, 64, 80]\n",
        "        depths = [2, 2, 2]\n",
        "        heads = [2, 2, 4]\n",
        "        patches = [2, 2, 2]\n",
        "    elif variant == \"xs\":\n",
        "        dims = [64, 80, 96]\n",
        "        depths = [2, 2, 2]\n",
        "        heads = [4, 4, 4]\n",
        "        patches = [2, 2, 2]\n",
        "    else:  # \"s\"\n",
        "        dims = [96, 128, 160]\n",
        "        depths = [2, 2, 3]\n",
        "        heads = [4, 4, 5]\n",
        "        patches = [2, 2, 2]\n",
        "\n",
        "    x = input_tensor\n",
        "    # Stem\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\", activation=\"swish\")(x)  # /2\n",
        "    x = layers.Conv2D(48, 3, strides=2, padding=\"same\", activation=\"swish\")(x)  # /4\n",
        "\n",
        "    # Stage 1\n",
        "    x = MobileViTBlock(dim=dims[0], patch_size=patches[0], depth=depths[0], num_heads=heads[0])(x)\n",
        "    x = layers.Conv2D(dims[0], 3, strides=2, padding=\"same\", activation=\"swish\")(x)  # /8\n",
        "\n",
        "    # Stage 2\n",
        "    x = MobileViTBlock(dim=dims[1], patch_size=patches[1], depth=depths[1], num_heads=heads[1])(x)\n",
        "    x = layers.Conv2D(dims[1], 3, strides=2, padding=\"same\", activation=\"swish\")(x)  # /16\n",
        "\n",
        "    # Stage 3\n",
        "    x = MobileViTBlock(dim=dims[2], patch_size=patches[2], depth=depths[2], num_heads=heads[2])(x)\n",
        "\n",
        "    return x  # feature map\n",
        "\n",
        "def build_mobilevit(input_shape, num_classes, variant=\"s\"):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    feats = build_mobilevit_backbone(inputs, variant=variant)\n",
        "    x = layers.GlobalAveragePooling2D()(feats)\n",
        "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)  # multi-label\n",
        "    model = Model(inputs, outputs, name=f\"mobilevit_{variant}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "yy4UZBCMSc4s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 6: Compile and Train the MobileViT Model\n",
        "# ==============================================================================\n",
        "tf.keras.backend.clear_session()\n",
        "mobilevit_model = build_mobilevit(input_shape=IMG_SIZE + (3,), num_classes=len(CLASSES), variant=\"s\")\n",
        "\n",
        "# --- Custom Weighted Binary Cross-Entropy Loss ---\n",
        "# Uses per-class weights derived from pos/neg counts calculated in Cell 3.\n",
        "pos_counts_tf = tf.constant(pos_counts, dtype=tf.float32)\n",
        "neg_counts_tf = tf.constant(neg_counts, dtype=tf.float32)\n",
        "pos_weight = neg_counts_tf / (pos_counts_tf + 1e-6)      # higher weight for rare positives\n",
        "neg_weight = tf.ones_like(pos_weight)                     # keep negatives weight = 1\n",
        "\n",
        "def weighted_bce(y_true, y_pred, smooth=0.05, eps=1e-7):\n",
        "    # Label smoothing for stability on small datasets\n",
        "    y_true = y_true * (1.0 - smooth) + 0.5 * smooth\n",
        "    y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n",
        "\n",
        "    # Broadcast weights: (batch, classes)\n",
        "    w_pos = pos_weight[tf.newaxis, :]\n",
        "    w_neg = neg_weight[tf.newaxis, :]\n",
        "\n",
        "    loss_pos = - w_pos * y_true * tf.math.log(y_pred)\n",
        "    loss_neg = - w_neg * (1.0 - y_true) * tf.math.log(1.0 - y_pred)\n",
        "    loss = loss_pos + loss_neg\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "mobilevit_model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(1e-4, weight_decay=1e-5),\n",
        "    loss=weighted_bce,\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
        "        tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "MOBILEVIT_MODEL_PATH = os.path.join(DATA_ROOT, \"mobilevit_skin_model.keras\")\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=7, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-6),\n",
        "    tf.keras.callbacks.ModelCheckpoint(MOBILEVIT_MODEL_PATH, monitor=\"val_auc\", mode=\"max\", save_best_only=True),\n",
        "]\n",
        "\n",
        "print(\"\\n💪 Starting MobileViT model training...\")\n",
        "history_mobilevit = mobilevit_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=50,                 # EarlyStopping will likely stop earlier\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhGccZshSfh3",
        "outputId": "3aa10953-8d39-4d09-8fd1-624cfa3d45ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💪 Starting MobileViT model training...\n",
            "Epoch 1/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1943s\u001b[0m 16s/step - acc: 0.5261 - auc: 0.5979 - loss: 1.1376 - precision: 0.2345 - recall: 0.6295 - val_acc: 0.7477 - val_auc: 0.8491 - val_loss: 0.9406 - val_precision: 0.4259 - val_recall: 0.8439 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 226ms/step - acc: 0.7468 - auc: 0.8354 - loss: 0.9156 - precision: 0.4197 - recall: 0.7997 - val_acc: 0.7724 - val_auc: 0.8534 - val_loss: 0.9123 - val_precision: 0.4536 - val_recall: 0.8148 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 232ms/step - acc: 0.7696 - auc: 0.8488 - loss: 0.8874 - precision: 0.4468 - recall: 0.7934 - val_acc: 0.7879 - val_auc: 0.8774 - val_loss: 0.8535 - val_precision: 0.4747 - val_recall: 0.8175 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 242ms/step - acc: 0.7866 - auc: 0.8649 - loss: 0.8477 - precision: 0.4698 - recall: 0.7907 - val_acc: 0.7884 - val_auc: 0.8785 - val_loss: 0.8471 - val_precision: 0.4752 - val_recall: 0.8095 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 245ms/step - acc: 0.7969 - auc: 0.8666 - loss: 0.8420 - precision: 0.4848 - recall: 0.7830 - val_acc: 0.7807 - val_auc: 0.8820 - val_loss: 0.8618 - val_precision: 0.4648 - val_recall: 0.8201 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 249ms/step - acc: 0.7980 - auc: 0.8777 - loss: 0.8156 - precision: 0.4869 - recall: 0.7864 - val_acc: 0.7957 - val_auc: 0.8953 - val_loss: 0.8335 - val_precision: 0.4862 - val_recall: 0.8360 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 260ms/step - acc: 0.7977 - auc: 0.8825 - loss: 0.8039 - precision: 0.4873 - recall: 0.7925 - val_acc: 0.8091 - val_auc: 0.9007 - val_loss: 0.8396 - val_precision: 0.5067 - val_recall: 0.8042 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 259ms/step - acc: 0.8396 - auc: 0.8959 - loss: 0.7641 - precision: 0.5605 - recall: 0.7975 - val_acc: 0.8571 - val_auc: 0.9152 - val_loss: 0.7665 - val_precision: 0.6012 - val_recall: 0.7937 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 261ms/step - acc: 0.8344 - auc: 0.9038 - loss: 0.7523 - precision: 0.5509 - recall: 0.7933 - val_acc: 0.8524 - val_auc: 0.9296 - val_loss: 0.7060 - val_precision: 0.5842 - val_recall: 0.8439 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 263ms/step - acc: 0.8536 - auc: 0.9209 - loss: 0.7163 - precision: 0.5857 - recall: 0.8400 - val_acc: 0.8545 - val_auc: 0.9307 - val_loss: 0.7095 - val_precision: 0.5889 - val_recall: 0.8413 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 260ms/step - acc: 0.8556 - auc: 0.9322 - loss: 0.6867 - precision: 0.5883 - recall: 0.8516 - val_acc: 0.8741 - val_auc: 0.9444 - val_loss: 0.6584 - val_precision: 0.6264 - val_recall: 0.8783 - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 262ms/step - acc: 0.8784 - auc: 0.9421 - loss: 0.6580 - precision: 0.6351 - recall: 0.8767 - val_acc: 0.8658 - val_auc: 0.9488 - val_loss: 0.6488 - val_precision: 0.6024 - val_recall: 0.9180 - learning_rate: 1.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 262ms/step - acc: 0.8840 - auc: 0.9458 - loss: 0.6422 - precision: 0.6455 - recall: 0.8898 - val_acc: 0.8922 - val_auc: 0.9529 - val_loss: 0.6444 - val_precision: 0.6680 - val_recall: 0.8889 - learning_rate: 1.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 262ms/step - acc: 0.8909 - auc: 0.9527 - loss: 0.6268 - precision: 0.6634 - recall: 0.8878 - val_acc: 0.8664 - val_auc: 0.9584 - val_loss: 0.6407 - val_precision: 0.6007 - val_recall: 0.9392 - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 265ms/step - acc: 0.8985 - auc: 0.9584 - loss: 0.6120 - precision: 0.6790 - recall: 0.9028 - val_acc: 0.8736 - val_auc: 0.9638 - val_loss: 0.6318 - val_precision: 0.6149 - val_recall: 0.9418 - learning_rate: 1.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9015 - auc: 0.9643 - loss: 0.5963 - precision: 0.6840 - recall: 0.9135 - val_acc: 0.9169 - val_auc: 0.9609 - val_loss: 0.6765 - val_precision: 0.7553 - val_recall: 0.8492 - learning_rate: 1.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 255ms/step - acc: 0.9152 - auc: 0.9676 - loss: 0.5856 - precision: 0.7282 - recall: 0.9035 - val_acc: 0.9087 - val_auc: 0.9627 - val_loss: 0.6475 - val_precision: 0.7289 - val_recall: 0.8466 - learning_rate: 1.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 263ms/step - acc: 0.9200 - auc: 0.9729 - loss: 0.5652 - precision: 0.7370 - recall: 0.9160 - val_acc: 0.9283 - val_auc: 0.9699 - val_loss: 0.5968 - val_precision: 0.7760 - val_recall: 0.8889 - learning_rate: 1.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 256ms/step - acc: 0.9226 - auc: 0.9756 - loss: 0.5515 - precision: 0.7389 - recall: 0.9312 - val_acc: 0.9262 - val_auc: 0.9692 - val_loss: 0.6103 - val_precision: 0.7652 - val_recall: 0.8968 - learning_rate: 1.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 257ms/step - acc: 0.9299 - auc: 0.9768 - loss: 0.5384 - precision: 0.7591 - recall: 0.9365 - val_acc: 0.9314 - val_auc: 0.9545 - val_loss: 0.6290 - val_precision: 0.8025 - val_recall: 0.8598 - learning_rate: 1.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 256ms/step - acc: 0.9345 - auc: 0.9780 - loss: 0.5362 - precision: 0.7746 - recall: 0.9348 - val_acc: 0.9241 - val_auc: 0.9488 - val_loss: 0.6364 - val_precision: 0.7596 - val_recall: 0.8942 - learning_rate: 1.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 264ms/step - acc: 0.9369 - auc: 0.9814 - loss: 0.5195 - precision: 0.7799 - recall: 0.9396 - val_acc: 0.9376 - val_auc: 0.9780 - val_loss: 0.5365 - val_precision: 0.7954 - val_recall: 0.9153 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 263ms/step - acc: 0.9405 - auc: 0.9822 - loss: 0.5149 - precision: 0.7954 - recall: 0.9348 - val_acc: 0.9365 - val_auc: 0.9783 - val_loss: 0.5324 - val_precision: 0.7904 - val_recall: 0.9180 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 258ms/step - acc: 0.9454 - auc: 0.9837 - loss: 0.5060 - precision: 0.8113 - recall: 0.9375 - val_acc: 0.9432 - val_auc: 0.9777 - val_loss: 0.5321 - val_precision: 0.8146 - val_recall: 0.9180 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 264ms/step - acc: 0.9429 - auc: 0.9828 - loss: 0.5118 - precision: 0.8029 - recall: 0.9360 - val_acc: 0.9334 - val_auc: 0.9787 - val_loss: 0.5383 - val_precision: 0.7724 - val_recall: 0.9339 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9468 - auc: 0.9866 - loss: 0.4929 - precision: 0.8125 - recall: 0.9442 - val_acc: 0.9474 - val_auc: 0.9782 - val_loss: 0.5306 - val_precision: 0.8209 - val_recall: 0.9339 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 261ms/step - acc: 0.9502 - auc: 0.9855 - loss: 0.4975 - precision: 0.8241 - recall: 0.9459 - val_acc: 0.9407 - val_auc: 0.9789 - val_loss: 0.5249 - val_precision: 0.7955 - val_recall: 0.9365 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 256ms/step - acc: 0.9510 - auc: 0.9868 - loss: 0.4918 - precision: 0.8236 - recall: 0.9516 - val_acc: 0.9370 - val_auc: 0.9778 - val_loss: 0.5404 - val_precision: 0.7936 - val_recall: 0.9153 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 263ms/step - acc: 0.9499 - auc: 0.9863 - loss: 0.4939 - precision: 0.8257 - recall: 0.9404 - val_acc: 0.9469 - val_auc: 0.9806 - val_loss: 0.5211 - val_precision: 0.8205 - val_recall: 0.9312 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 264ms/step - acc: 0.9496 - auc: 0.9867 - loss: 0.4920 - precision: 0.8281 - recall: 0.9348 - val_acc: 0.9360 - val_auc: 0.9811 - val_loss: 0.5297 - val_precision: 0.7785 - val_recall: 0.9392 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9503 - auc: 0.9881 - loss: 0.4861 - precision: 0.8242 - recall: 0.9461 - val_acc: 0.9360 - val_auc: 0.9809 - val_loss: 0.5307 - val_precision: 0.7810 - val_recall: 0.9339 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 265ms/step - acc: 0.9514 - auc: 0.9883 - loss: 0.4857 - precision: 0.8257 - recall: 0.9506 - val_acc: 0.9499 - val_auc: 0.9816 - val_loss: 0.5225 - val_precision: 0.8386 - val_recall: 0.9206 - learning_rate: 5.0000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 256ms/step - acc: 0.9537 - auc: 0.9891 - loss: 0.4792 - precision: 0.8336 - recall: 0.9520 - val_acc: 0.9494 - val_auc: 0.9768 - val_loss: 0.5349 - val_precision: 0.8415 - val_recall: 0.9127 - learning_rate: 5.0000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 257ms/step - acc: 0.9570 - auc: 0.9895 - loss: 0.4748 - precision: 0.8455 - recall: 0.9526 - val_acc: 0.9386 - val_auc: 0.9742 - val_loss: 0.5672 - val_precision: 0.8106 - val_recall: 0.8942 - learning_rate: 5.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 264ms/step - acc: 0.9525 - auc: 0.9897 - loss: 0.4801 - precision: 0.8307 - recall: 0.9493 - val_acc: 0.9505 - val_auc: 0.9826 - val_loss: 0.5137 - val_precision: 0.8341 - val_recall: 0.9312 - learning_rate: 5.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 264ms/step - acc: 0.9572 - auc: 0.9889 - loss: 0.4738 - precision: 0.8459 - recall: 0.9533 - val_acc: 0.9453 - val_auc: 0.9828 - val_loss: 0.5334 - val_precision: 0.8192 - val_recall: 0.9233 - learning_rate: 5.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 258ms/step - acc: 0.9630 - auc: 0.9899 - loss: 0.4636 - precision: 0.8655 - recall: 0.9583 - val_acc: 0.9494 - val_auc: 0.9819 - val_loss: 0.5181 - val_precision: 0.8398 - val_recall: 0.9153 - learning_rate: 5.0000e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 258ms/step - acc: 0.9545 - auc: 0.9901 - loss: 0.4720 - precision: 0.8373 - recall: 0.9507 - val_acc: 0.9479 - val_auc: 0.9828 - val_loss: 0.5200 - val_precision: 0.8337 - val_recall: 0.9153 - learning_rate: 5.0000e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9604 - auc: 0.9899 - loss: 0.4661 - precision: 0.8577 - recall: 0.9543 - val_acc: 0.9443 - val_auc: 0.9786 - val_loss: 0.5318 - val_precision: 0.8140 - val_recall: 0.9259 - learning_rate: 5.0000e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 256ms/step - acc: 0.9643 - auc: 0.9924 - loss: 0.4495 - precision: 0.8660 - recall: 0.9649 - val_acc: 0.9412 - val_auc: 0.9817 - val_loss: 0.5389 - val_precision: 0.8099 - val_recall: 0.9127 - learning_rate: 2.5000e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 256ms/step - acc: 0.9676 - auc: 0.9933 - loss: 0.4418 - precision: 0.8774 - recall: 0.9682 - val_acc: 0.9453 - val_auc: 0.9808 - val_loss: 0.5487 - val_precision: 0.8317 - val_recall: 0.9021 - learning_rate: 2.5000e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 257ms/step - acc: 0.9677 - auc: 0.9934 - loss: 0.4396 - precision: 0.8813 - recall: 0.9634 - val_acc: 0.9494 - val_auc: 0.9813 - val_loss: 0.5415 - val_precision: 0.8483 - val_recall: 0.9021 - learning_rate: 2.5000e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 263ms/step - acc: 0.9730 - auc: 0.9944 - loss: 0.4282 - precision: 0.8980 - recall: 0.9711 - val_acc: 0.9494 - val_auc: 0.9846 - val_loss: 0.5191 - val_precision: 0.8318 - val_recall: 0.9286 - learning_rate: 1.2500e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9741 - auc: 0.9953 - loss: 0.4231 - precision: 0.9020 - recall: 0.9724 - val_acc: 0.9520 - val_auc: 0.9842 - val_loss: 0.5232 - val_precision: 0.8417 - val_recall: 0.9286 - learning_rate: 1.2500e-05\n",
            "Epoch 45/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 256ms/step - acc: 0.9760 - auc: 0.9956 - loss: 0.4192 - precision: 0.9108 - recall: 0.9713 - val_acc: 0.9515 - val_auc: 0.9841 - val_loss: 0.5258 - val_precision: 0.8397 - val_recall: 0.9286 - learning_rate: 1.2500e-05\n",
            "Epoch 46/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 258ms/step - acc: 0.9771 - auc: 0.9958 - loss: 0.4156 - precision: 0.9144 - recall: 0.9729 - val_acc: 0.9520 - val_auc: 0.9819 - val_loss: 0.5312 - val_precision: 0.8434 - val_recall: 0.9259 - learning_rate: 1.2500e-05\n",
            "Epoch 47/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9768 - auc: 0.9960 - loss: 0.4153 - precision: 0.9133 - recall: 0.9726 - val_acc: 0.9427 - val_auc: 0.9805 - val_loss: 0.5314 - val_precision: 0.8027 - val_recall: 0.9365 - learning_rate: 6.2500e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9776 - auc: 0.9965 - loss: 0.4110 - precision: 0.9139 - recall: 0.9762 - val_acc: 0.9469 - val_auc: 0.9790 - val_loss: 0.5323 - val_precision: 0.8161 - val_recall: 0.9392 - learning_rate: 6.2500e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 257ms/step - acc: 0.9800 - auc: 0.9967 - loss: 0.4082 - precision: 0.9213 - recall: 0.9808 - val_acc: 0.9484 - val_auc: 0.9766 - val_loss: 0.5346 - val_precision: 0.8203 - val_recall: 0.9418 - learning_rate: 6.2500e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 258ms/step - acc: 0.9785 - auc: 0.9967 - loss: 0.4084 - precision: 0.9155 - recall: 0.9798 - val_acc: 0.9505 - val_auc: 0.9813 - val_loss: 0.5222 - val_precision: 0.8294 - val_recall: 0.9392 - learning_rate: 3.1250e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 7: Evaluate the MobileViT Model on the Test Set\n",
        "# ==============================================================================\n",
        "# No custom layers are required to reload this model (only standard Keras layers),\n",
        "# but we include the custom loss for completeness if you want to load with compile=True.\n",
        "custom_objects = {\n",
        "    \"MobileViTBlock\": MobileViTBlock,\n",
        "    \"weighted_bce\": weighted_bce\n",
        "}\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(MOBILEVIT_MODEL_PATH, custom_objects=custom_objects, compile=False)\n",
        "loaded_model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(1e-4, weight_decay=1e-5),\n",
        "    loss=weighted_bce,\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
        "        tf.keras.metrics.AUC(name=\"auc\", multi_label=True),\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\"),\n",
        "    ],\n",
        ")\n",
        "print(\"✅ MobileViT model loaded successfully!\")\n",
        "\n",
        "print(\"\\n🔬 Evaluating the final MobileViT on the unseen test set...\")\n",
        "test_results = loaded_model.evaluate(test_ds)\n",
        "\n",
        "print(\"\\n--- Final Test Set Evaluation Results (MobileViT) ---\")\n",
        "for metric, value in zip(loaded_model.metrics_names, test_results):\n",
        "    print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPfoX1EzhIl-",
        "outputId": "312d1154-72b9-4844-b0a7-35fccad2e37c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MobileViT model loaded successfully!\n",
            "\n",
            "🔬 Evaluating the final MobileViT on the unseen test set...\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 13s/step - acc: 0.9497 - auc: 0.9818 - loss: 0.5092 - precision: 0.8216 - recall: 0.9372\n",
            "\n",
            "--- Final Test Set Evaluation Results (MobileViT) ---\n",
            "loss: 0.5316\n",
            "compile_metrics: 0.9447\n"
          ]
        }
      ]
    }
  ]
}